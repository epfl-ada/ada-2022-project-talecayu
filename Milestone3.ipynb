{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = pd.read_csv(\"data/MovieSummaries/plot_summaries.txt\", sep='\\t', header=None, names=[\"wikiID\", \"plot\"])\n",
    "\n",
    "df_meta = pd.read_csv(\"data/MovieSummaries/movie.metadata.tsv\", sep='\\t', header=None, \n",
    "    names=[\"wikiID\", \"freeID\", \"name\", \"release_date\", \"revenue\", \"runtime\", \"languages\", \"countries\", \"genres\"])\n",
    "\n",
    "df_char = pd.read_csv(\"data/MovieSummaries/character.metadata.tsv\", sep='\\t',header=None, \n",
    "    names=[\"WikiID\", \"freeID\", \"release_date\", \"char_name\", \"actor_DOB\", \"actor_gender\", \"actor_height\", \"actor_ethnicity\",\n",
    "           \"actor_name\", \"Actor_age\", \"freeID_char_map\", \"FreeID_char\", \"FreeID_actor\"])\n",
    "\n",
    "df_char_names = pd.read_csv('data/MovieSummaries/name.clusters.txt', sep=\"\\t\", header=None, \n",
    "    names=[\"char_name\", \"freeID_char_map\"])\n",
    "\n",
    "df_tropes = pd.read_csv('data/MovieSummaries/tvtropes.clusters.txt', sep='\\t', header=None, names=[\"trope\", \"details\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data features and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the output above, we might think that the languages, countries and genres columns have no missing elements but actually some elements are just empty dictionary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_meta['languages'].value_counts()['{}'])\n",
    "print(df_meta['countries'].value_counts()['{}'])\n",
    "print(df_meta['genres'].value_counts()['{}'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, if we want to make use of the release date we can already convert it to datetime and check if we have more missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta['release_date'] = pd.to_datetime(df_meta['release_date'], errors = 'coerce')\n",
    "print(df_meta['release_date'].shape[0] - df_meta['release_date'].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char_names.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tropes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tropes = df_tropes.drop([\"details\"], axis=1).join(pd.json_normalize(df_tropes[\"details\"].map(json.loads).tolist())).rename(\n",
    "    columns={\"id\":\"freeID_char_map\"})\n",
    "\n",
    "df_tropes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tropes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## NLPCore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we looked at the Stanford CoreNLP-processed summaries, we assumed it would not be useful for us.\n",
    "\n",
    "The code below is an example where we load and parse the data to retrieve an element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "path = \"data/corenlp_plot_summaries/\"\n",
    "files = os.listdir(path)\n",
    "print(files[0])\n",
    "f = gzip.open(path+files[0], 'rb')\n",
    "\n",
    "test = f.read().decode()\n",
    "# print(test)\n",
    "\n",
    "Bs_data = BeautifulSoup(test, \"xml\")\n",
    "print(Bs_data.find(\"sentence\", {'id':'20'}).find('NER').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genres, languages and countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to identify the most represented genres, languages and countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcount(df):\n",
    "    df2 = pd.DataFrame(data={\n",
    "        'id': df.apply(lambda x: list(json.loads(x).keys())),\n",
    "        'name': df.apply(lambda x: list(json.loads(x).values()))\n",
    "    })\n",
    "    \n",
    "    distinctf = []\n",
    "    counter = []\n",
    "\n",
    "    for i in range(df2.shape[0]):\n",
    "        fid = df2[\"id\"].iloc[i]\n",
    "        fname = df2[\"name\"].iloc[i]\n",
    "        for j in range(len(fid)):\n",
    "            if [fid[j], fname[j]] not in distinctf:\n",
    "                distinctf.append([fid[j], fname[j]])\n",
    "                counter.append(1)\n",
    "            else:\n",
    "                counter[distinctf.index([fid[j], fname[j]])] += 1\n",
    "\n",
    "    new = pd.DataFrame(data={\n",
    "        'id': [s[0] for s in distinctf],\n",
    "        'name': [s[1] for s in distinctf],\n",
    "        'count': counter\n",
    "    })\n",
    "    \n",
    "    return new.sort_values(by=\"count\", ascending=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genre = fcount(df_meta[\"genres\"])\n",
    "df_genre.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lang = fcount(df_meta[\"languages\"])\n",
    "df_lang.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country = fcount(df_meta[\"countries\"])\n",
    "df_country.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There might be a correlation between the country and language features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Release date distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta.groupby(df_meta[\"release_date\"].dt.year).count()['wikiID'].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to study the similarity between two movies so the date of their releases can be interesting. In this case, the exact release date might not be necessary, because it is expected that the difference would lie between movies that have for example 10 years of difference not just a few months. Actually, in some cases we only have the year of the release, so it might be simpler to only store the year of the release for all movies in order to keep homogeneous data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_meta['runtime'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 years of runtime?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_meta[df_meta['runtime'] >= 400].shape[0])\n",
    "print(df_meta[df_meta['runtime'] <= 3].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More than 6 hours of runtime seems too large and less than 3 minutes seems too short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.hist(df_meta[df_meta['runtime'] < 400]['runtime'], bins=400)\n",
    "ax.set_title('Distribution of runtime')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the distribution is more concentrated around a runtime of 90 minutes but it appears there is a cluster around 20 which might corresponds to short films (6th most represented movie genre in the dataset).\n",
    "\n",
    "The exact value of the runtime does not always seem to be reliable data, and looking at the distribution it might be a good idea to replace the runtime by an attribute like \"short\", \"long\" or \"don't know\" (if runtime is NaN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta['revenue'].dropna().hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_meta['revenue'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_meta[df_meta['revenue']==df_meta['revenue'].max()].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a very large different between most of the movies and the movies that had a huge success such as \"Avatar\".\n",
    "\n",
    "Note that the box office revenue is absent for most of the movies and we might not need it to compute the similarity between two movies. Therefore, we estimated that this feature would not be useful for our project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of the number of words per plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use raw plot_summaries and count nb of words in each plot \n",
    "df_plot_fltr = df_plot.copy()\n",
    "df_plot_fltr['nb_words'] = df_plot_fltr['plot'].apply(lambda n: len(n.split()))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.hist(df_plot_fltr['nb_words'], bins=1000)\n",
    "ax.set_title('Distribution of the number of words per plot')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.hist(df_plot_fltr['nb_words'], bins=1000)\n",
    "ax.set_title('Zoom between 0 and 100 words')\n",
    "plt.xlim([0, 100])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still find some extreme values in the distribution. In particular, some plots have less than 20 words, on the other hand there are plots with thousands of words.\n",
    "\n",
    "In the preprocessing, we decided to discard the movies with plots that are less than 10 words as they might not be relevant when we will apply topic extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of the number of characters per movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will group all the characters by movie. So if a movie has at least one labeled character, we will be able to count its number of character so that we can have an insight on the distribution of the number of characters for a movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char_fltr = df_char[['freeID','FreeID_actor']].copy()\n",
    "df_char_fltr = df_char_fltr.groupby(['freeID']).size()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.hist(df_char_fltr, bins=120)\n",
    "ax.set_title('Distribution of the number of labeled characters per movie')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of movies without any labeled character: \", \n",
    "      df_meta.shape[0] - df_meta[df_meta['freeID'].isin(df_char_fltr.index)].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be missing information but it is also logical that some movies such as documentaries do not have any character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_square = dict(markerfacecolor='r', marker='s')\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.boxplot(df_char_fltr, vert=False, flierprops=red_square)\n",
    "ax.set_title('Boxplot for the number of labeled characters per movie')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1rst quartile, median and 3rd quartile values: \")\n",
    "print(df_char_fltr.quantile([0.25,0.5,0.75]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join the dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature connecting dataframes together is the Wikipedia ID. Also there are more metadatas of movies (81741 movies) than plots (42303 movies). We will only keep the metadatas of the movies we know the plot of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfull = df_meta.merge(df_plot, how='inner', on=\"wikiID\")\n",
    "dfull.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only a small fraction of the characters have been labellised with a trope (500), compared to the number of unlabeled (450669). Here is the dataframe containing the characters with trope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char_trope = df_char.merge(df_tropes, how='inner', on='freeID_char_map')\n",
    "\n",
    "df_char_trope.drop(columns=[\"char\", \"actor\"], inplace=True)\n",
    "df_char_trope.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char_list = df_char[['freeID','char_name']].groupby('freeID')['char_name'].apply(list).reset_index(name='characters')\n",
    "df_char_list.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actor_list = df_char[['freeID','actor_name']].groupby('freeID')['actor_name'].apply(list).reset_index(name='actors')\n",
    "df_actor_list.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to filter our data, and remove the elements without a genre or with less than 10 words in their plot, because these are the features that we think are the most important in the idea of our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = (dfull[(dfull['genres'] != '{}') & (dfull['plot'].apply(lambda s: len(s.split())) > 10)]).drop(\n",
    "    columns=['revenue'])\n",
    "\n",
    "print('number of movies remaining after movie metadata preprocessing: ', df_clean.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are formatting the data to facilitate the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['countries'] = pd.DataFrame(data={'countries': df_clean['countries'].apply(lambda x: set(json.loads(x).values()))})\n",
    "df_clean['languages'] = pd.DataFrame(data={'languages': df_clean['languages'].apply(lambda x: set(json.loads(x).values()))})\n",
    "df_clean['genres'] = pd.DataFrame(data={'genres': df_clean['genres'].apply(lambda x: set(json.loads(x).values()))}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to replace the full release date by the year of release, so that all the movies have only an integer for this feature, and because we estimated that the year was sufficient to establish a similarity on the release date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['release_date'] = df_clean['release_date'].dt.year.apply(lambda x: int(x) if x == x else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to redefine the runtime either as \"short\" for movies with a runtime less than 40 minutes (a short film is generally less than 40 minutes), \"long\" for movies with a runtime larger than 40 minutes or \"unknown\" if we do not know the runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['runtime'] = df_clean['runtime'].apply(lambda x: 'short' if x <= 40 else ('long' if x > 40 else None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add the actors list and character list to the df_clean dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.merge(df_char_list, how='inner', on='freeID')\n",
    "df_clean = df_clean.merge(df_actor_list, how='inner', on='freeID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['actors'] =pd.DataFrame(data={'actors': df_clean['actors'].apply(lambda x: list(x))})\n",
    "df_clean['actors'] =pd.DataFrame(data={'actors': df_clean['actors'].apply(lambda x: [item for item in x if not(pd.isna(item)) == True] )})\n",
    "df_clean['actors'] =pd.DataFrame(data={'actors': df_clean['actors'].apply(lambda x: set(x) )})\n",
    "df_clean['characters'] =pd.DataFrame(data={'characters': df_clean['characters'].apply(lambda x: list(x))})\n",
    "df_clean['characters'] =pd.DataFrame(data={'characters': df_clean['characters'].apply(lambda x: [item for item in x if not(pd.isna(item)) == True] )})\n",
    "df_clean['characters'] =pd.DataFrame(data={'characters': df_clean['characters'].apply(lambda x: set(x))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "print(df_clean['characters'][1])\n",
    "print(pd.isna(df_clean['characters'][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already performed test to scrape data with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pywikibot\n",
    "from pywikibot import *\n",
    "\n",
    "def scraping(wiki_ID):\n",
    "    list = []\n",
    "\n",
    "    query_base = \"https://query.wikidata.org/bigdata/namespace/wdq/sparql?format=json&query=\"\n",
    "\n",
    "    wiki_ID_encoded = wiki_ID.replace(\"/\",\"%2F\")\n",
    "\n",
    "    pre_str = ( \"https://query.wikidata.org/bigdata/namespace/wdq/sparql?format=json&query=PREFIX%20wd%3A%20%3Chttp%3A%2F%2\" +\n",
    "        \"Fwww.wikidata.org%2Fentity%2F%3E%0APREFIX%20wdt%3A%20%3Chttp%3A%2F%2Fwww.wikidata.org%2Fprop%2Fdirect%2F%3E%0APREF\" +\n",
    "        \"IX%20wikibase%3A%20%3Chttp%3A%2F%2Fwikiba.se%2Fontology%23%3E%0A%0ASELECT%20%20%3Fs%20%3FsLabel%20%3Fp%20%20%3Fo%2\" +\n",
    "        \"0%3FoLabel%20WHERE%20%7B%0A%20%3Fs%20wdt%3AP646%20%22\" )\n",
    "    post_str = ( \"%22%20%0A%0A%20%20%20SERVICE%20wikibase%3Alabel%20%7B%0A%20%20%20%20bd%3AserviceParam%20wikibase%3Alangua\" +\n",
    "        \"ge%20%22en%22%20.%0A%20%20%20%7D%0A%20%7D\" )\n",
    "\n",
    "    query = pre_str + wiki_ID_encoded + post_str\n",
    "\n",
    "    response = requests.get(query)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    results = soup.find(id=\"results\")\n",
    "\n",
    "    try:\n",
    "        entity_wiki_id = response.json()['results']['bindings'][0]['s']['value']\n",
    "    except:\n",
    "        print(\"Scraping failed\")\n",
    "        return [None,None]\n",
    "        \n",
    "\n",
    "    entity_wiki_id = response.json()['results']['bindings'][0]['s']['value']\n",
    "\n",
    "    str = entity_wiki_id.split('/')\n",
    "    entity = str[-1]\n",
    "    print(entity)\n",
    "    site = pywikibot.Site(\"wikidata\", \"wikidata\")\n",
    "    repo = site.data_repository()\n",
    "    item = pywikibot.ItemPage(repo, entity)\n",
    "    item_dict = item.get()\n",
    "    clm_dict = item_dict[\"claims\"] # Get the claim dictionary\n",
    "\n",
    "    #P57 for a movie director\n",
    "    try:\n",
    "        clm_list = clm_dict[\"P57\"]\n",
    "        for clm in clm_list:\n",
    "                ...\n",
    "                clm_trgt = clm.getTarget()\n",
    "        list.append(clm_trgt.labels['en'])\n",
    "    except:\n",
    "        list.append(None)\n",
    "\n",
    "    #P462 for the color\n",
    "    try:\n",
    "        clm_list = clm_dict[\"P462\"]\n",
    "        for clm in clm_list:\n",
    "                ...\n",
    "                clm_trgt = clm.getTarget()\n",
    "        list.append(clm_trgt.labels['en'])\n",
    "    except:\n",
    "        list.append(None)\n",
    "\n",
    "    print('scraped sucessfuly')\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = df_clean\n",
    "\n",
    "list2 = [[]]\n",
    "for index, row in sub_df.iterrows():\n",
    "    list2.append(scraping(row['freeID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scrapped = pd.DataFrame (list2, columns = ['director', 'color'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scrapped.to_csv('scrapped.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scrapped = pd.read_csv('scrapped.csv')\n",
    "df_scrapped.drop(columns = df_scrapped.columns[0], axis = 1, inplace= True)\n",
    "df_scrapped.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_clean = df_clean.join(df_scrapped)\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Graph construction\n",
    "\n",
    "The aim of the project is to build a weighted graph connecting all movies together. There are two ways to represent a graph:\n",
    "- Adjacency Matrix\n",
    "- Adjacency List\n",
    "\n",
    "However, as calculated in the README, storing the Adjacency Matrix will be about 3.6 GB if we assume each weight is a float. We believe this file is too large, so we will represent it as an adjacency list, and for each vertex we will store the top 100 (for example) most similar movies. This implies that the graph will be directed. Also, we take more than the number of movies we will recommend directly because we want to be able to apply some filters in the recommandation tool (by language for example) without having to recompute the whole graph. For the compute time, we will have approximately 42'306×42'305÷2 = 894'877'665 comparisons to make, which we're confident we'll be able to compute in a reasonable time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Choice of attributes based on data exploratory analysis\n",
    "\n",
    "We try to have a maximum of useful attributes to estimate the similarity between the films of the dataset, and to keep as much movies as possible to maximize the chance of being able to recommand similar movies.\n",
    "\n",
    "When we \"filter\" an attribute, we remove all instances that are missing that attribute's information.\n",
    "\n",
    "We believe that the mandatory attribute conditions to be retained in our final dataset are:\n",
    "* A present title: without a film name, there is no film (hard to name it, to advise it)\n",
    "* At least one genre labeled (probably one of the most significant attributes for similarity)\n",
    "* A plot of at least 10 existing words (impose a minimum size so that the subsequent analysis of the topics is accurate)\n",
    "\n",
    "We eliminate the 'revenue' attribute, due to the small number of movies described by this attribute.\n",
    "\n",
    "The attributes that are not filtered, but kept to help with the information they carry are:\n",
    "* Runtime\n",
    "* Release date\n",
    "* Language: will be ignored if not provided\n",
    "* Country: will be ignored if not provided\n",
    "* Characters: Will be ignored if not provided.\n",
    "\n",
    "By exploring the wikidata pages by scraping, we recover the name of the producer and the information on the color of the film. These two attributes\n",
    "were the most constantly present among other more scattered ones (filming location, cinematographer, composer, etc.). We therefore create an additional dataset thanks to the code present in the \"scraping\" section above.\n",
    "\n",
    "We discretize the `date_release` in years of publication. The runtime is transformed into short/long categories.\n",
    "\n",
    "Among the attributes selected for further analysis, we can create attributes resulting from a multiplication between two attributes, such as color and release date.\n",
    "\n",
    "We have these attributes, with their possible values if categorized in `df_clean`:\n",
    "- `Plot`\n",
    "- `Title`\n",
    "- `Release_date`\n",
    "- `Genres`: genres list\n",
    "- `Countries`: countries list\n",
    "- `Language`: languages list\n",
    "- `Runtime`: `short`, `long` or `don't know`\n",
    "- `Director`: Director name or `None`\n",
    "- `Color`: colored, black and white or `None`\n",
    "\n",
    "In `df_clean_char`, we holded only the actors names and character names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Methods to study similarities accross chosen attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Computing the global similarity between movies taking into account all attributes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following parts, we will see methods to compute the respective similarity between movies based on each respective feature. We will want to merge all these similarities into one global similarity metric that will take into accounts all attributes.\n",
    "We compute the total similarity between two films as followed:\n",
    "$$\n",
    "S_{tot}=\\sum_{i}^{N} w_iS_i\n",
    "$$\n",
    "where $w_i$ is the weight associated to the similarity $S_i \\in [0,1]$ of the attribute $i \\in $ {plot, title, genre, runtime, release_year, country, language, actors, characters, director, color }."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Algorithm to study similarity between movies attributes\n",
    "\n",
    "In this part we will propose several methods in order to assess the similarity between movies based on the attributes that we chose here-above. In the [Annexe: Mathematical definitions related to similarity](#Annexe:-Mathematical-definitions-related-to-similarity) we explain the mathematical details and definitions on which the following algorithms are based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import seaborn as sns\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard similarity implementation\n",
    "**Method**\n",
    "The jaccard similarity is more appropriate to compute similarities between datasets of categorical attributes where we want to see if these datasets have some attributes in common or not, and how many they are.\n",
    "\n",
    "**Implementation**\n",
    "`jaccard_similarity` function written here below. Since we compare to what extent different movies attributes contain the exact same words, we may need to add a lemmatizer for example to take care of same words being in the singular or plural form, or having uppercase or lowercase initial letter.\n",
    "\n",
    "**Movie Attribute similarity that can be studied**\n",
    "- Movie genre\n",
    "- Movie Language \n",
    "- Movie Country\n",
    "- Actors\n",
    "- Characters\n",
    "- Director\n",
    "\n",
    "Here we can use this method for these attributes because we want to find which of those attributes contain the exact same words and study the similarity based on this analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(x,y):\n",
    "  \"\"\" returns the jaccard similarity between two sets x and y\"\"\"\n",
    "  intersection_cardinality = len(set.intersection(*[x, y]))\n",
    "  union_cardinality = len(set.union(*[x,y]))\n",
    "  if union_cardinality == 0:\n",
    "    return 0.0\n",
    "  else:\n",
    "    return intersection_cardinality/float(union_cardinality)\n",
    "\n",
    "\n",
    "list1 = ['Drama', 'Mystery', 'Fantasy', 'Adventure']\n",
    "list2 = ['Fantasy Adventure', 'Adventure', 'Epic', 'Fantasy', 'Drama', 'Action', 'Sword and sorcery films']\n",
    "list3= ['Satire', 'Comedy', 'Family film']\n",
    "list4= ['Musical', 'Drama', 'Comedy']\n",
    "list5= ['Romantic comedy', 'Romance Film', 'Comedy']\n",
    "list6= ['Comedy']\n",
    "list7=[]\n",
    "list8= ['Action']\n",
    "list9=[]\n",
    "\n",
    "print(\"Example of similarities computed with jaccard similarity for the attribute -Movie genre- :\")\n",
    "print(\"Between lists 1 and 2:\",jaccard_similarity(set(list1), set(list2)))\n",
    "print(\"Between lists 3 and 4:\",jaccard_similarity(set(list3), set(list4)))\n",
    "print(\"Between lists 5 and 6:\",jaccard_similarity(set(list5), set(list6)))\n",
    "print(\"Between lists 3 and 6:\",jaccard_similarity(set(list3), set(list6)))\n",
    "print(\"Between lists 3 and 7:\",jaccard_similarity(set(list3), set(list7)))\n",
    "print(\"Between lists 2 and 8:\",jaccard_similarity(set(list2), set(list8)))\n",
    "print(\"Between lists 7 and 9:\",jaccard_similarity(set(list7), set(list9)))\n",
    "print(\"Between lists 6 and 8:\",jaccard_similarity(set(list6), set(list8)))\n",
    "print(\"Between lists 8 and 8:\",jaccard_similarity(set(list8), set(list8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity between binary attributes\n",
    "**Method**\n",
    "\n",
    "We have a unique discrete attribute that can take the values: $\\left\\{1, 0, \\text{Nan} \\right\\}$, where NaN is the case where we don't have information about this attribute.  \n",
    "\n",
    "**Implementation**\n",
    "\n",
    "The similarity here is simply 1 if both attributes are equal to 1 or both equal to 0. If at least one of the two attributes is Nan then the similarity is 0.\n",
    "\n",
    "**Movie Attribute similarity that can be studied**\n",
    "\n",
    "- Color (1: Movie in colours, 0: Movie in black and white, NaN)\n",
    "- Runtime (1: full-length film, 0: short film, NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isNaN(string):\n",
    "    return string != string\n",
    "\n",
    "def binary_similarity(x,y):\n",
    "    '''\n",
    "    returns the similarity in values of \n",
    "    0 or 1\n",
    "    '''\n",
    "    if(isNaN(x) | isNaN(y)):\n",
    "        return 0\n",
    "    if(x==y):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity between positive integers attributes\n",
    "**Method**\n",
    "We want compute the similiarity between two one-dimension continuous variables that belong to $\\mathbb{N}$. \n",
    "\n",
    "**Implementation**\n",
    "We start by computing the distance between the two positive integer by computing the manhattan distance: \n",
    "$$\n",
    "d(A,B)=|A-B|\n",
    "$$\n",
    "We want to normalize this distance so that $d \\in [0,1]$. We do a Min-Max normalization that performs a linear transformation on the original data. This technique gets all the scaled data in the range $[0,1]$. This allows to preserve the relationships among the original data values. The formula to achieve this is the following:\n",
    "$$\n",
    "d_{scaled}=\\frac{d-d_{min}}{d_{max}-d_{min}}\n",
    "$$\n",
    "The cost of having this bounded range is that we will end up with smaller standard deviations, which can suppress the effect of outliers.\n",
    "We can finally compute the similarity based on this distance:\n",
    "$$\n",
    "S=1-d_{scaled}\n",
    "$$\n",
    "where $S \\in [0,1]$.\n",
    "\n",
    "**Movie Attribute similarity that can be studied**\n",
    "- Release year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integer_similarity(distance_matrix):\n",
    "    '''\n",
    "    inputs: matrice of distances\n",
    "    output: similarity associated to normalized manhattan distance\n",
    "    '''\n",
    "\n",
    "    integ_similiarity=np.zeros(distance_matrix.shape)\n",
    "    \n",
    "    idx=np.where(~np.isnan(distance_matrix))\n",
    "    idx2=np.where(np.isnan(distance_matrix))\n",
    "\n",
    "    d_max = np.max(distance_matrix[idx])\n",
    "    d_min = np.min(distance_matrix[idx])\n",
    "    \n",
    "    integ_similiarity[idx]=np.ones(distance_matrix[idx].shape) - (distance_matrix[idx]-d_min)/(d_max-d_min)    \n",
    "    integ_similiarity[idx2]=0\n",
    "    \n",
    "    return integ_similiarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf-idf : Term frequency-inverse document frequency\n",
    "**Term frequency and Inverse document frequency**\n",
    "\n",
    "$$\n",
    "tf(t,d)=\\frac{f_{t,d}}{ \\sum_{t' \\in d} f_{t',d} } \\quad \\textrm{and} \\quad idf(t,D)=log(\\frac{N}{|d\\in D:t\\in d|})\n",
    "$$\n",
    "where $f_{t,d}$ = raw frequency = number of times a term $t$ occurs in document $d$, $ \\sum_{t' \\in d} f_{t',d}$=total number of terms $t'$ in $d$ by suming each independent occurrence\n",
    "<br>\n",
    "where $N$=$|D|$= number of documents in corpus $D$, $|d\\in D:t\\in d|$= number of documents where t appears (where $tf(t,d)\\neq 0$)\n",
    "\n",
    "To prevent bias towards longer documents, the term frequency can be computed as follow:\n",
    "$$\n",
    "tf(t,d)=0.5+0.5\\frac{f_{t,d}}{max(f_{t',d}:t'\\in d)}\n",
    "$$\n",
    "To avoid divinding by zero, the idf denominator can ba adjusted as follow:\n",
    "$$\n",
    "idf(t,D)=log(\\frac{N}{1+|d\\in D:t\\in d|})\n",
    "$$\n",
    "**Term frequency-inverse document frequency**\n",
    "\n",
    "From the term frequency and the inverse document frequency we can compute the tf-idf:\n",
    "$$\n",
    "tf-idf(t,D)=tf(t,d) \\cdot idf(t,D)\n",
    "$$\n",
    "\n",
    "The tf-idf reflects how important a word is to a document in corpus. tf-idf increase proportionally with the number of times a word appear in a document, but it is offset by the number of documents in the corpus that contains the word. It is high when there is high term frequency in document d **and** low document frequency of the term in the whole corpus. This helps to adjust the fact that some words appear more frequently that we can define as common terms. It tends to filter out those common words.\n",
    "\n",
    "**Implementation**\n",
    "\n",
    "tfidf can be implemented using the `sklearn` or `GenSim` opensource library. After obtaining tf-idf associated for each word in each text, we can associate to each text a vector composed of his words tf-idf and null values for other words.\n",
    "Then we can compute the **cosine similarity** between those texts represented as vectors.\n",
    "\n",
    "**Movie Attribute similarity that can be studied**\n",
    "- Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(A,B): \n",
    "  return (sum(a*b for a,b in zip(A,B)))\n",
    "\n",
    "def cosine_similarity_simple(a,b):\n",
    "  \"\"\" returns cosine similarity between two lists \"\"\"\n",
    "  return dot(a,b)/((dot(a,a)**.5)*(dot(b,b)**.5)) \n",
    "\n",
    "print(\"Example cosine similarity between vectors [1,0] and [1,1]: \", cosine_similarity_simple([1,0],[1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import sub\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.downloader as api\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.similarities import WordEmbeddingSimilarityIndex\n",
    "from gensim.similarities import SparseTermSimilarityMatrix\n",
    "from gensim.similarities import SoftCosineSimilarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "#df_clean[\"name\"].head()\n",
    "Counter(\" \".join(df_clean[\"name\"]).split()).most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_headlines=['the','of','the','and','in','a','to','my','for','on','&','from']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def headlines_similarity(df_headlines, list_stopwords):\n",
    "    \n",
    "    documents = list(df_headlines)\n",
    "    vectorizer = sklearn.feature_extraction.text.TfidfVectorizer(lowercase=True,stop_words=list_stopwords)\n",
    "    X = vectorizer.fit_transform(documents)\n",
    "    list_vectors = X.toarray()\n",
    "    headlines_similarity=sklearn.metrics.pairwise.cosine_similarity(list_vectors)\n",
    "    \n",
    "    return headlines_similarity\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1= 'Harry Potter and the Chamber of Secrets'\n",
    "list2= 'Gnomes and Trolls: The Secret Chamber'\n",
    "list3= 'Harry Potter and the Half-Blood Prince'\n",
    "list4= 'The Taking of Prince Harry'\n",
    "list5= 'The Prince and Me'\n",
    "list6= 'Despicable Me'\n",
    "list7= 'Just Me and You'\n",
    "list8= 'The Secret Life of Bees'\n",
    "headlines = [list1,list2,list3,list4,list5,list6,list7,list8]\n",
    "\n",
    "def create_heatmap(similarity, labels, cmap = \"YlGnBu\"):\n",
    "  df = pd.DataFrame(similarity)\n",
    "  df.columns = labels\n",
    "  df.index = labels\n",
    "  fig, ax = plt.subplots()\n",
    "  sns.heatmap(df, cmap=cmap)\n",
    "\n",
    "testsimilarity = headlines_similarity(headlines, stopwords_headlines)\n",
    "print(\"Example cosine similarities between 8 movie titles: \\n\", testsimilarity)\n",
    "\n",
    "# Output the sorted similarity scores and documents\n",
    "sorted_indexes = np.argsort(testsimilarity[0])[::-1]\n",
    "print(\"Similarities computed between the title 'Harry Potter and the Chamber of Secrets' and the other titles:\")\n",
    "for idx in sorted_indexes:\n",
    "    print(idx,'\\t', testsimilarity[0,idx],'\\t', headlines[idx])\n",
    "  \n",
    "create_heatmap(testsimilarity, headlines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GloVe algorithm\n",
    "**Definition**\n",
    "The `GloVe` algorithm is an opensource standford algorithm that associates similarity between words by obtaining vectors representations for words and by mapping them into a meaningful space.\n",
    "\n",
    "**Implementation** \n",
    "We can apply soft cosine similarity computation to study similarity between texts. The matrix S from the soft cosine definition can be calculated with the library `GenSim` and using the `GloVe` algorithm. We would obtain a dataset containing a vector for all the words in our corpus and we could compute the matrix S associating a similarity matrix to all of these words. Then with the soft cosine similarity we could analyse the similarity between the documents in our corpus. \n",
    "\n",
    "**Movie Attribute similarity that can be studied**\n",
    "- Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import sub\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.downloader as api\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.similarities import WordEmbeddingSimilarityIndex\n",
    "from gensim.similarities import SparseTermSimilarityMatrix\n",
    "from gensim.similarities import SoftCosineSimilarity\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "download(\"stopwords\")  # Download stopwords list.\n",
    "\n",
    "# Load the model: this is a big file, can take a while to download and open\n",
    "glove = api.load('glove-wiki-gigaword-50') \n",
    "#glove = api.load('text8')  # download the corpus and return it opened as an iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc, stopwords):\n",
    "    # From: https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb\n",
    "    # Tokenize, clean up input document string\n",
    "    doc = sub(r'<img[^<>]+(>|$)', \" image_token \", doc)\n",
    "    doc = sub(r'<[^<>]+(>|$)', \" \", doc)\n",
    "    doc = sub(r'\\[img_assist[^]]*?\\]', \" \", doc)\n",
    "    doc = sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \" url_token \", doc)\n",
    "    return [token for token in simple_preprocess(doc, min_len=0, max_len=float(\"inf\")) if token not in stopwords]\n",
    "\n",
    "def compute_similarity_index(model_name):\n",
    "    model = api.load(model_name)     \n",
    "    similarity_index = WordEmbeddingSimilarityIndex(model)\n",
    "    return similarity_index\n",
    "\n",
    "def headlines_soft_similarity(df_headlines, stopwords, similarity_index):\n",
    "    # From: https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb\n",
    "    \n",
    "    documents = list(df_headlines)\n",
    "    headlines_similarity=np.zeros([len(documents),len(documents)])\n",
    "\n",
    "    # Preprocess the documents\n",
    "    corpus = [preprocess(document,stopwords) for document in documents]\n",
    "    dictionary = Dictionary(corpus)\n",
    "    tfidf = TfidfModel(dictionary=dictionary)\n",
    "    # Create the term similarity matrix.  \n",
    "    similarity_matrix = SparseTermSimilarityMatrix(similarity_index, dictionary, tfidf)\n",
    "\n",
    "    for i in range(len(documents)):\n",
    "        query_tf = tfidf[dictionary.doc2bow(corpus[i])]\n",
    "        index = SoftCosineSimilarity(tfidf[[dictionary.doc2bow(document) for document in corpus]],similarity_matrix)\n",
    "        headlines_similarity[i,:] = index[query_tf]\n",
    "\n",
    "    return headlines_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_headlines=['the','of','the','and','in','a','to','my','for','on','&','from']\n",
    "similarity_index = compute_similarity_index('glove-wiki-gigaword-50') # or 'text8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_headlines = df_clean['name']\n",
    "list1= 'Harry Potter and the Chamber of Secrets'\n",
    "list2= 'Gnomes and Trolls: The Secret Chamber'\n",
    "list3= 'Harry Potter and the Half-Blood Prince'\n",
    "list4= 'The Taking of Prince Harry'\n",
    "list5= 'The Prince and Me'\n",
    "list6= 'Despicable Me'\n",
    "list7= 'Just Me and You'\n",
    "list8= 'The Secret Life of Bees'\n",
    "headlines = [list1, list2,list3,list4,list5,list6,list7,list8]\n",
    "\n",
    "stopwords1=[]\n",
    "test_without_stopwords = headlines_soft_similarity(headlines, stopwords1, similarity_index)\n",
    "\n",
    "stopwords2=stopwords_headlines\n",
    "test_few_stopwords = headlines_soft_similarity(headlines, stopwords2, similarity_index)\n",
    "\n",
    "stopwords3 = set(stopwords.words(\"english\"))\n",
    "test_full_stopwords = headlines_soft_similarity(headlines, stopwords3, similarity_index)\n",
    "\n",
    "# Output the sorted similarity scores and documents\n",
    "sorted_indexes = np.argsort(test_without_stopwords[0])[::-1]\n",
    "print(\"Soft similarities computed between the title 'Harry Potter and the Chamber of Secrets' and the other titles:\")\n",
    "for idx in sorted_indexes:\n",
    "    print(idx,'\\t', test_without_stopwords[0,idx],'\\t',headlines[idx])\n",
    "\n",
    "create_heatmap(test_without_stopwords,headlines)\n",
    "\n",
    "# Output the sorted similarity scores and documents\n",
    "sorted_indexes = np.argsort(test_few_stopwords[0])[::-1]\n",
    "print(\"Soft similarities computed between the title 'Harry Potter and the Chamber of Secrets' and the other titles:\")\n",
    "for idx in sorted_indexes:\n",
    "    print(idx,'\\t', test_few_stopwords[0,idx],'\\t',headlines[idx])\n",
    "\n",
    "create_heatmap(test_few_stopwords,headlines)\n",
    "\n",
    "# Output the sorted similarity scores and documents\n",
    "sorted_indexes = np.argsort(test_full_stopwords[0])[::-1]\n",
    "print(\"Soft similarities computed between the title 'Harry Potter and the Chamber of Secrets' and the other titles:\")\n",
    "for idx in sorted_indexes:\n",
    "    print(idx,'\\t', test_full_stopwords[0,idx],'\\t',headlines[idx])\n",
    "\n",
    "create_heatmap(test_full_stopwords,headlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from nltk.corpus import stopwords\n",
    "#from nltk import download\n",
    "#download(\"stopwords\")  # Download stopwords list.\n",
    "\n",
    "stopwords_headlines=['the','of','the','and','in','a','to','my','for','on','&','from']\n",
    "similarity_index = compute_similarity_index('glove-wiki-gigaword-50')\n",
    "#similarity_index = compute_similarity_index('glove-wiki-gigaword-100')\n",
    "#similarity_index = compute_similarity_index('text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create similarity matrices for each similarity matrice\n",
    "\n",
    "def similarity_fct(df):\n",
    "    '''jacc_genre = np.zeros((df.shape[0], df.shape[0]))\n",
    "    jacc_language = np.zeros((df.shape[0], df.shape[0]))\n",
    "    jacc_country = np.zeros((df.shape[0], df.shape[0]))\n",
    "    jacc_actors = np.zeros((df.shape[0], df.shape[0])).astype('uint16')\n",
    "    jacc_characters = np.zeros((df.shape[0], df.shape[0])).astype('uint16')\n",
    "    bin_director = np.zeros((df.shape[0], df.shape[0]))\n",
    "    bin_color = np.zeros((df.shape[0], df.shape[0]))\n",
    "    bin_runtime = np.zeros((df.shape[0], df.shape[0]))\n",
    "    integ_date = np.zeros((df.shape[0], df.shape[0]))\n",
    "    cosine_headlines = np.zeros((df.shape[0], df.shape[0])).astype('uint16')'''\n",
    "    #integ_date = np.zeros((df.shape[0], df.shape[0])).astype('uint16')\n",
    "    idx_nan = np.isnan(df[\"release_date\"].to_numpy())\n",
    "    all_idx = np.repeat(True, df.shape[0])\n",
    "    print(\"idenxes done\")\n",
    "    diff_date = np.tile(df[\"release_date\"].to_numpy().astype('int16'), (df.shape[0],1))\n",
    "    print('stockage_created')\n",
    "    diff_date = diff_date - diff_date.transpose()\n",
    "    diff_date = np.abs(diff_date)\n",
    "    print('difference calculated')\n",
    "    diff_date[np.ix_(idx_nan, all_idx)] = -1\n",
    "    diff_date[np.ix_(all_idx, idx_nan)] = -1\n",
    "    print('replaced nan')\n",
    "    #diff_date[idx_nan][:] = -1\n",
    "    #print(diff_date)\n",
    "    #idx_no_nan = np.where(~np.isnan(diff_date))\n",
    "    #print('created idx no_nan')\n",
    "    #idx_nan    = np.where( np.isnan(diff_date))\n",
    "    #print('created idx nan')\n",
    "\n",
    "    diff_date_max = np.max(diff_date)\n",
    "\n",
    "    print(diff_date_max)\n",
    "    \n",
    "    diff_date[np.ix_(idx_nan, all_idx)] = diff_date_max\n",
    "    diff_date[np.ix_(all_idx, idx_nan)] = diff_date_max\n",
    "    \n",
    "    print('replaced nan')\n",
    "    \n",
    "    integ_date = np.zeros((df.shape[0], df.shape[0]), dtype = 'uint16')\n",
    "    \n",
    "    print('stockage_created 2')\n",
    "    \n",
    "    integ_date = 65535 - (diff_date*65535/diff_date_max).astype('uint16')\n",
    "    np.fill_diagonal(integ_date, 65535)\n",
    "    #soft_cosine_headlines = np.zeros((df.shape[0], df.shape[0]))\n",
    "    ID_index = []\n",
    "\n",
    "    integ_date.tofile(\"integ_date\")\n",
    "    matrices_dict = {}\n",
    "    \n",
    "\n",
    "    '''for i in range(len(df)):\n",
    "        #print('i',i)\n",
    "        for j in range(i+1,len(df)):\n",
    "            #fill only the half of the similarity matrices\n",
    "            jacc_characters[i,j] = jaccard_similarity(df['characters'].iloc[i],df['characters'].iloc[j]) * 65535\n",
    "        print(i)\n",
    "    jacc_characters += jacc_characters.transpose()\n",
    "    np.fill_diagonal(jacc_characters, 65535)\n",
    "    jacc_characters.tofile(\"jacc_characters\")\n",
    "    print(\"jacc_characters done\")\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        #print('i',i)\n",
    "        for j in range(i+1,len(df)):\n",
    "            #fill only the half of the similarity matrices\n",
    "            jacc_actors[i,j] = jaccard_similarity(df['actors'].iloc[i],df['actors'].iloc[j]) * 65535\n",
    "        print(i)\n",
    "    jacc_actors += jacc_actors.transpose()\n",
    "    np.fill_diagonal(jacc_actors, 65535)\n",
    "    jacc_actors.tofile(\"jacc_actors\")\n",
    "    print(\"jacc_actors done\")'''\n",
    "    \n",
    "    #cosine_headlines = (headlines_similarity(df['name'], stopwords_headlines) * 65535).astype('uint16')\n",
    "    #cosine_headlines.tofile(\"cosine_headlines\")\n",
    "    #print(\"cosine_headlines done\")\n",
    "    \n",
    "    '''for i in range(len(df)):\n",
    "        ID_index.append(df['name'][i])\n",
    "        #print('i',i)\n",
    "        for j in range(len(df)):\n",
    "            #fill only the half of the similarity matrices\n",
    "            if(i<j):\n",
    "                jacc_genre[i,j] = jaccard_similarity(df['genres'].iloc[i],df['genres'].iloc[j])\n",
    "                jacc_language[i,j] = jaccard_similarity(df['languages'].iloc[i],df['languages'].iloc[j])\n",
    "                jacc_country[i,j] = jaccard_similarity(df['countries'].iloc[i],df['countries'].iloc[j])\n",
    "                jacc_actors[i,j] = jaccard_similarity(df['actors'].iloc[i],df['actors'].iloc[j])\n",
    "                jacc_characters[i,j] = jaccard_similarity(df['characters'].iloc[i],df['characters'].iloc[j])\n",
    "                bin_director[i,j] = binary_similarity(df['director'].iloc[i],df['director'].iloc[j])\n",
    "                bin_color[i,j]= binary_similarity(df['color'].iloc[i],df['color'].iloc[j])\n",
    "                bin_runtime[i,j]= binary_similarity(df['runtime'].iloc[i],df['runtime'].iloc[j])\n",
    "\n",
    "                if( np.isnan(df['release_date'].iloc[i]) | np.isnan(df['release_date'].iloc[j]) ):\n",
    "                    integ_date[i,j] = None\n",
    "                else:\n",
    "                    integ_date[i,j]=abs(df['release_date'].iloc[i]-df['release_date'].iloc[j])\n",
    "\n",
    "    jacc_genre = jacc_genre+jacc_genre.transpose()\n",
    "    np.fill_diagonal(jacc_genre, 1)\n",
    "\n",
    "    jacc_language = jacc_language+jacc_language.transpose()\n",
    "    np.fill_diagonal(jacc_language, 1)\n",
    "\n",
    "    jacc_country = jacc_country+jacc_country.transpose()\n",
    "    np.fill_diagonal(jacc_country, 1)  \n",
    "\n",
    "    jacc_actors += jacc_actors.transpose()\n",
    "    np.fill_diagonal(jacc_actors, 1)\n",
    "\n",
    "    jacc_characters += jacc_characters.transpose()\n",
    "    np.fill_diagonal(jacc_characters, 1)\n",
    "\n",
    "    bin_director += bin_director.transpose()\n",
    "    np.fill_diagonal(bin_director, 1)\n",
    "\n",
    "    bin_color += bin_color.transpose()\n",
    "    np.fill_diagonal(bin_color, 1)\n",
    "\n",
    "    bin_runtime += bin_runtime.transpose()\n",
    "    np.fill_diagonal(bin_runtime, 1)\n",
    "    \n",
    "    integ_date = integ_date+ integ_date.transpose()\n",
    "    integ_date = integer_similarity(integ_date)    \n",
    "    np.fill_diagonal(integ_date, 1)\n",
    "\n",
    "    cosine_headlines = headlines_similarity(df['name'], stopwords_headlines)\n",
    "    soft_cosine_headlines = headlines_soft_similarity(df['name'], stopwords_headlines, similarity_index)\n",
    "\n",
    "    matrices_dict['genre'] = jacc_genre\n",
    "    matrices_dict['language'] = jacc_language\n",
    "    matrices_dict['countries'] = jacc_country\n",
    "    matrices_dict['date'] = integ_date\n",
    "    matrices_dict['actors'] = jacc_actors\n",
    "    matrices_dict['characters'] = jacc_characters\n",
    "    matrices_dict['director'] = bin_director\n",
    "    matrices_dict['color'] = bin_color\n",
    "    matrices_dict['runtime'] = bin_runtime\n",
    "    matrices_dict['headlines'] = cosine_headlines\n",
    "    matrices_dict['headlines_soft'] = soft_cosine_headlines'''\n",
    "    matrices_dict['date'] = integ_date\n",
    "\n",
    "    return matrices_dict, ID_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sub_df = df_clean.loc[0:7]\n",
    "sub_df = df_clean\n",
    "sub_df.reset_index(drop=True,inplace=True)\n",
    "sub_df.head(10)\n",
    "sim_dict, ID_vect =similarity_fct(df=sub_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_dict['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_sim = np.triu(sim_dict['date'], 1)\n",
    "print(\"upper_sim\")\n",
    "#indices = np.transpose(np.nonzero(upper_sim))\n",
    "largest_edges = upper_sim*(upper_sim>=65000)\n",
    "indices = np.transpose(np.nonzero(largest_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = 20\n",
    "i = 0\n",
    "print(len(indices))\n",
    "for test_idx in range(len(indices)):\n",
    "    #if(len(df_clean[\"characters\"].iloc[indices[test_idx][0]]) >= 3 and \n",
    "    #   len(df_clean[\"characters\"].iloc[indices[test_idx][1]]) >= 3 ):\n",
    "    print(indices[test_idx])\n",
    "    print(df_clean[\"name\"].iloc[indices[test_idx][0]], df_clean[\"release_date\"].iloc[indices[test_idx][0]])\n",
    "    print(df_clean[\"name\"].iloc[indices[test_idx][1]], df_clean[\"release_date\"].iloc[indices[test_idx][1]])\n",
    "    i += 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df2 = df_clean.loc[0:20]\n",
    "sub_df2.reset_index(drop=True,inplace=True)\n",
    "sub_df2.head(10)\n",
    "sim_dict2, ID_vect2 =similarity_fct(df=sub_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut pas de NAn dans les similarity matrixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, (key, value) in enumerate(sim_dict.items()):\n",
    "    print('NaN count for',key,'similarity matrix is', np.count_nonzero(np.isnan(value)))\n",
    "    #print(value.shape)\n",
    "    print(np.max(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, (key, value) in enumerate(sim_dict2.items()):\n",
    "    print('NaN count for',key,'similarity matrix is', np.count_nonzero(np.isnan(value)))\n",
    "    #print(value.shape)\n",
    "    print(np.max(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity matrix te une seule similarité (ici le genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#matrices_dict['genre'] = jacc_genre\n",
    "#matrices_dict['language'] = jacc_language\n",
    "#matrices_dict['countries'] = jacc_country\n",
    "#matrices_dict['date'] = integ_date\n",
    "#matrices_dict['actors'] = jacc_actors\n",
    "#matrices_dict['characters'] = jacc_characters\n",
    "#matrices_dict['director'] = bin_director\n",
    "#matrices_dict['color'] = bin_color\n",
    "#matrices_dict['runtime'] = bin_runtime\n",
    "#matrices_dict['headlines'] = cosine_headlines\n",
    "#matrices_dict['headlines_soft'] = soft_cosine_headlines\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "cax = ax.matshow(sim_dict2['headlines'] , interpolation='nearest')\n",
    "ax.grid(True)\n",
    "plt.title('Film Similarity matrix')\n",
    "plt.xticks(range(len(sub_df2)), ID_vect2, rotation=90)\n",
    "plt.yticks(range(len(sub_df2)), ID_vect2)\n",
    "fig.colorbar(cax) #ticks=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, .75,.8,.85,.90,.95,1]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "  \n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "cax = ax.matshow(sim_dict['headlines_soft'], interpolation='nearest')\n",
    "ax.grid(True)\n",
    "plt.title('Film Similarity matrix')\n",
    "plt.xticks(range(len(sub_df)), ID_vect, rotation=90)\n",
    "plt.yticks(range(len(sub_df)), ID_vect)\n",
    "fig.colorbar(cax, ticks=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, .75,.8,.85,.90,.95,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity matrix with weight (not yet normalized, not all similarites yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sim_dict))\n",
    "#weighted similarity fct\n",
    "w1 = 1\n",
    "w2 = 1\n",
    "w3 = 1\n",
    "w4 = 1\n",
    "w5 = 1\n",
    "w6 = 1\n",
    "w7 = 1\n",
    "w8 = 1\n",
    "w9 = 1\n",
    "w10 = 1\n",
    "w = [w1,w2,w3,w4,w5,w6,w7,w8,w9]# w10] added when added distance\n",
    "w_norm = w / np.sum(w)\n",
    "print(w_norm)\n",
    "sim_matrix = np.zeros(sim_dict['language'].shape)\n",
    "\n",
    "for index, (key, value) in enumerate(sim_dict.items()):\n",
    "    sim_matrix = sim_matrix + w_norm[index]*value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "  \n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "cax = ax.matshow(sim_matrix, interpolation='nearest')\n",
    "ax.grid(True)\n",
    "plt.title('Film Similarity matrix')\n",
    "plt.xticks(range(len(sub_df)), ID_vect, rotation=90, fontsize=10)\n",
    "plt.yticks(range(len(sub_df)), ID_vect, fontsize=10)\n",
    "fig.colorbar(cax, ticks=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, .75,.8,.85,.90,.95,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Analysis similarity metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> étudier déplacement des clusters en fonction permet de voir quels films influencent quel films? \n",
    "- quels pays influencent quel pays?\n",
    "- quels genres influencent quels genres?\n",
    "- quels topics apparaissent disparaissent? trends dans le cinéma? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "cax = ax.matshow(sim_dict['genre'][0:100,0:100], interpolation='nearest')\n",
    "ax.grid(True)\n",
    "plt.title('Film Similarity matrix for one feature')\n",
    "#plt.xticks(range(len(sub_df)), ID_vect[0:20], rotation=90)\n",
    "#plt.yticks(range(len(sub_df)), ID_vect[0:20])\n",
    "fig.colorbar(cax, ticks=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, .75,.8,.85,.90,.95,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset for analysis testing (dataset of 1000 films)\n",
    "df_test_1000=sub_df.copy()\n",
    "sim_dict_test_1000, ID_vect_1000 = [sim_dict, ID_vect] \n",
    "features=['genre','language','countries','date','actors','characters','director','color','runtime','headlines']\n",
    "\n",
    "sim_feature=sim_dict_test_1000['genre']\n",
    "\n",
    "print(type(sim_feature))\n",
    "print(type(ID_vect_1000))\n",
    "print(np.shape(sim_feature))\n",
    "print(np.shape(df_test_1000['genres']))\n",
    "df_test_1000.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess df and associated similarity matrix to remove movies that don't have a release date\n",
    "index = np.where(df_test_1000['release_date'].isna())[0]\n",
    "df_test_1000=df_test_1000[df_test_1000['release_date'].notna()].reset_index(drop=True)\n",
    "sim_feature=np.delete(sim_feature,index,0)\n",
    "sim_feature=np.delete(sim_feature,index,1)\n",
    "\n",
    "print(np.shape(sim_feature))\n",
    "print(np.shape(df_test_1000))\n",
    "df_test_1000.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#algo clustering sur similarité\n",
    "print(df_clean['release_date'].min())\n",
    "print(df_clean['release_date'].max())\n",
    "print(df_test_1000['release_date'].min())\n",
    "print(df_test_1000['release_date'].max())\n",
    "df_test_1000['decade'] = df_test_1000.copy()['release_date'].apply(lambda x: x-x%10)\n",
    "df_test_1000.groupby(df_test_1000['release_date']).count()['wikiID'].plot()\n",
    "plt.show()\n",
    "df_test_1000.groupby(df_test_1000['decade']).count()['wikiID'].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each decade show similarity\n",
    "df_decades=df_test_1000.groupby(df_test_1000['decade'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=0\n",
    "decade_similarity={}            #each key is a decade (ex: 1940-1949 or 1990-1999)\n",
    "decade_similarity_additive={}   #each key is from the beginning until a decade (ex: 1890-1849 or 1890-1889)\n",
    "filter_index_tot=[]\n",
    "\n",
    "for decade, group in df_decades:\n",
    "    filter_index=group.index.tolist()\n",
    "    filter_index.sort()\n",
    "    decade_similarity[str(int(decade))+'-'+str(int(decade+9))]=sim_feature[np.ix_(filter_index,filter_index)]\n",
    "    filter_index_tot.extend(filter_index)\n",
    "    filter_index_tot.sort()\n",
    "    decade_similarity_additive['until '+str(int(decade+9))]=sim_feature[np.ix_(filter_index_tot,filter_index_tot)]\n",
    "    \"\"\"\n",
    "    #print(filter_index_tot)\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    cax = ax.matshow(decade_similarity[str(int(decade))+'-'+str(int(decade+9))][0:100,0:100], interpolation='nearest')\n",
    "    ax.grid(True)\n",
    "    plt.title('Film Similarity matrix for one feature')\n",
    "    #plt.xticks(range(len(sub_df)), ID_vect[0:20], rotation=90)\n",
    "    #plt.yticks(range(len(sub_df)), ID_vect[0:20])\n",
    "    fig.colorbar(cax, ticks=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, .75,.8,.85,.90,.95,1])\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    cax = ax.matshow(decade_similarity_additive['until '+str(int(decade+9))][0:100,0:100], interpolation='nearest')\n",
    "    ax.grid(True)\n",
    "    plt.title('Film Similarity matrix for one feature')\n",
    "    #plt.xticks(range(len(sub_df)), ID_vect[0:20], rotation=90)\n",
    "    #plt.yticks(range(len(sub_df)), ID_vect[0:20])\n",
    "    fig.colorbar(cax, ticks=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, .75,.8,.85,.90,.95,1])\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "\n",
    "print(decade_similarity.keys())\n",
    "print(decade_similarity_additive.keys())\n",
    "#print(decade_similarity.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "### 3. Topic extraction method\n",
    "To compare plots, we will use a topic extraction algorithm. There are a lot that exist, but we will concentrate on three possible: LDA, Doc2Vec and BERTopic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing method on a document corpus before topic extraction\n",
    "\n",
    "Here is a preprocessing approach that can be implemented before using a topic extraction analysis\n",
    "1. Removing the stop words from the documents which are the most common words occuring in texts that give no additional concept. It can be done with `Java` using `MySQL`.\n",
    "2. Removing the numbers appart from years, the non-asci characters, and most common occuring names (ex: James, Robert, John)\n",
    "3. Handle pural and singular form of the same word by lemmatizing.\n",
    "4. We can filter the words using tf-idf. We can compute the tf-idf for each word of each plot and keep the words with highest tf-idf score. The threshold for a word to be kept has to be defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA: Latent Dirichlet Allocation\n",
    "\n",
    "<div>\n",
    "<img src=\"images/LDA_example.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "**Definition**\n",
    "\n",
    "LDA is an algorithm that can be used for topic extraction in texts. It is an unsupervised machine-learning model that takes documents as input and finds topics as output. A topic is represented as a weighted list of words. The model also says in what percentage each document talks about each topic. \n",
    "\n",
    "**Implementation**\n",
    "\n",
    "LDA can be implemented using the `GenSim` library. When using LDA to analyse topics in a corpus, it needs some preprocessing steps before applying the algorithm to make it more efficient. We detailed one preprocessing approach above.\n",
    "\n",
    "\n",
    "After this processing we can apply LDA on our database by tuning some parameters:\n",
    "<br>\n",
    "$K$: the number of topics we look for\n",
    "<br>\n",
    "$\\alpha$: K-dimension vector of positive reals that represent the prior weights of topic K in a document which affects the document-topic distribution. \n",
    "<br>\n",
    "$\\eta$: V-dimension vector of positive reals that represents the prior weights of each words in topics which affects the topic-word distribution\n",
    "\n",
    "If we chose a symetric LDA, the weights $\\alpha$ would be the same for all topics and the weights $\\eta$ would be the same for all words in a topic. The smaller the $\\alpha$ the fewer topics per document, the fewer the $\\eta$ the fewer words per topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc2Vec topic extraction\n",
    "**Definition**\n",
    "\n",
    "Doc2Vec is an unsupervised algorithm that learns fixed-length feature vectors for paragraphs/documents/texts. Then we can compare these vectors to assess the similarity between documents. Doc2vec allows to generate a semantic space which is a spatial space where distance among vectors are indicator of semantic similarity. This semantic space consisting of word and document vectors is a continuous representation of topics, unlike LDA where topics are sampled from a discrete space. It means that the dense areas having high concentration of document can be thought of having similar topics and can be best represented by nearby embedded words.\n",
    "\n",
    "**Implementation**\n",
    "\n",
    "It can be implemented using the `GenSim` library with the class `Doc2Vec` that extends the class `Word2Vec`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERTopic\n",
    "\n",
    "<div>\n",
    "<img src=\"images/BERTopic.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "**Definition**\n",
    "BERTopic is a guided semi-supervised topic modeling algorithm. Contrary to LDA, it uses embeddings (thus semi-supervised) and class-based tf-idf (ctf-idf) to find easily interpretable topics and makes it more stable to small variations. It works in three stages:\n",
    "\n",
    "- Embed the documents\n",
    "- Cluster the documents\n",
    "- Create a list of topics and their representation\n",
    "\n",
    "With this algorithm it is possible to find a list of topics and also give the probability of each topic in each document.\n",
    "\n",
    "**Implementation**\n",
    "\n",
    "BERTopic can be implemented using the python `BerTopic` library. As with LDA, we can preprocess the documents (plots in our case) before giving it to the BERTopic for better results.\n",
    "\n",
    "Each stage of the BERTopic can be parameterized to get better results:\n",
    "<br>\n",
    "$Embedding$: by default it uses the `paraphrase-MiniLM-L6-v2` sentence transformers, but we can use any other embedding technique.\n",
    "<br>\n",
    "$Clustering$: by default it uses HDBSCAN (hierarchical DBSCAN) to cluster the documents, but to apply this efficiently UMAP is used to reduce the dimensionality of the embeddings. It is possible to change the dimension reduction algorithm and the clustering algorithm, for example we could import K-means form `sklearn.cluster` and use that.\n",
    "<br>\n",
    "$Topic$ $representation$: by default it uses class-based tf-idf, meaning it adjusts the tf-idf inside each class (cluster) in relation with the frequency of a word inside the class. Here again it can be tuned for example to try to show different representatives of a cluster. This can be useful to avoid having \"bycycle\", \"cycling\" and \"bike\" as the representatives of a cluster, where only one of them would have been enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computation of similarity from topic extraction\n",
    "\n",
    "Regardless of the method of analysis of the plot, we can have a vector representation at the end:\n",
    "- Doc2Vec gives a vector directly\n",
    "- LDA gives the freqency of each topic per document. We can build a vector for each document where each element represents a topic, and it will be proportional to the frequency of the topic.\n",
    "- BERTopic gives a probability of each topic per document. Similarly to LDA we can build a vector where each element is a topic and fill it with the corresponding probability.\n",
    "\n",
    "We can then compare these vectors using the cosine similarity for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#convert to list \n",
    "docs = df_clean[\"plot\"].to_list()\n",
    "\n",
    "#stopwords\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\")\n",
    "\n",
    "# Prepare embeddings\n",
    "sentence_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "embeddings = sentence_model.encode(docs, show_progress_bar = True)\n",
    "print(\"embedded\")\n",
    "\n",
    "\n",
    "# Create topic model\n",
    "model = BERTopic(vectorizer_model=vectorizer_model, verbose = True, calculate_probabilities=True, nr_topics=50)\n",
    "print(\"model created\")\n",
    "topics, probabilities = model.fit_transform(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model \n",
    " \n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\")\n",
    "\n",
    "model = BERTopic(vectorizer_model=vectorizer_model, verbose=True, calculate_probabilities=True)\n",
    " \n",
    "#convert to list \n",
    "docs = df_clean[\"plot\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, probabilities = model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_topic_freq()\n",
    "\n",
    "#df_plot_topics = df_plot.copy()\n",
    "#df_plot_topics = df_plot_topics.drop([\"plot\"], axis=1)\n",
    "\n",
    "#df_plot_topics = pd.DataFrame(model.probabilities_)\n",
    "#print(model.probabilities_.shape)\n",
    "#df_plot_topics[\"wikiID\"] = df_plot[\"wikiID\"]\n",
    "\n",
    "#df_plot_topics[\"topics\"] = np.zeros(model.probabilities_.shape[0])\n",
    "#df_plot_topics[\"topics\"] = df_plot[\"topics\"].astype('object')\n",
    "\n",
    "#for i in range(model.probabilities_.shape[0]):\n",
    "#    df_plot_topics.at[i,\"topics\"] = model.probabilities_[i]\n",
    "#df_plot_topics\n",
    "#model.probabilities_.tofile(\"model_probs\")\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_distribution(probabilities[0], min_probability = 0.0, height = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.get_topic_freq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"bertopic_model_new_embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = BERTopic.load(\"bertopic_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.get_topic_freq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_documents(docs, embeddings = embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_term_rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_barchart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_probabilities = np.fromfile(\"model_probs\", dtype = 'float')\n",
    "length = model_probabilities.shape[0]\n",
    "model_probabilities = model_probabilities.reshape(37476, int(length/37476))\n",
    "print(model_probabilities.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_vectors = model_probabilities\n",
    "list_vectors = embeddings\n",
    "print(\"copied\")\n",
    "print(list_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plots_similarity = sklearn.metrics.pairwise.cosine_similarity(list_vectors)\n",
    "print(\"cos calculated\")\n",
    "plots_similarity *= 65535\n",
    "print(\"normalized\")\n",
    "plots_similarity2 = plots_similarity.astype('uint16', copy=False)\n",
    "print(\"converted\", plots_similarity2.dtype)\n",
    "plots_similarity2.tofile(\"plots_similarity\")\n",
    "print(\"saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_similarity2 = np.fromfile(\"plots_similarity\", dtype = 'uint16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots_similarity2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_sim = np.triu(plots_similarity2, 1)\n",
    "threshold_sim = 50000\n",
    "largest_edges = upper_sim*(upper_sim>=threshold_sim)\n",
    "indices = np.transpose(np.nonzero(largest_edges))\n",
    "\n",
    "i = 0\n",
    "print(len(indices))\n",
    "for test_idx in range(len(indices)):\n",
    "    #if(len(df_clean[\"characters\"].iloc[indices[test_idx][0]]) >= 3 and \n",
    "    #   len(df_clean[\"characters\"].iloc[indices[test_idx][1]]) >= 3 ):\n",
    "    print(indices[test_idx])\n",
    "    print(df_clean[\"name\"].iloc[indices[test_idx][0]], df_clean[\"release_date\"].iloc[indices[test_idx][0]], df_clean[\"wikiID\"].iloc[indices[test_idx][0]])\n",
    "    print(df_clean[\"name\"].iloc[indices[test_idx][1]], df_clean[\"release_date\"].iloc[indices[test_idx][1]], df_clean[\"wikiID\"].iloc[indices[test_idx][1]])\n",
    "    i += 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_sim[8364][25341]\n",
    "#df_clean[\"plot\"].iloc[87]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded genre\n",
      "genre in b\n",
      "loaded plot\n",
      "plot in b\n",
      "loaded headline\n",
      "headline in b\n"
     ]
    }
   ],
   "source": [
    "a = np.fromfile(\"../similarities/jacc_genre\", dtype = 'uint16')\n",
    "a = a.reshape(37476, 37476)\n",
    "print(\"loaded genre\")\n",
    "b = np.zeros((37476,37476)).astype('uint16')\n",
    "b += (0.45*a).astype('uint16')\n",
    "print(\"genre in b\")\n",
    "a = np.fromfile(\"../similarities/plots_similarity_embeddings\", dtype = 'uint16')\n",
    "a = a.reshape(37476, 37476)\n",
    "print(\"loaded plot\")\n",
    "b += (0.45*a).astype('uint16')\n",
    "print(\"plot in b\")\n",
    "a = np.fromfile(\"../similarities/cosine_headlines\", dtype = 'uint16')\n",
    "a = a.reshape(37476, 37476)\n",
    "print(\"loaded headline\")\n",
    "b += (0.1*a).astype('uint16')\n",
    "print(\"headline in b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkit as nk\n",
    "\n",
    "#remove diag and lower triangle\n",
    "upper_sim = np.triu(b, 1)\n",
    "\n",
    "print(\"calculated upper tri\")\n",
    "\n",
    "#keep above threshold\n",
    "threshold_sim = 45000\n",
    "largest_edges = upper_sim*(upper_sim>=threshold_sim)\n",
    "\n",
    "print(\"selected largest edges\")\n",
    "\n",
    "#find indices of largest values\n",
    "indices = np.transpose(np.nonzero(largest_edges))\n",
    "\n",
    "print(\"nb_edges:\", len(indices))\n",
    "\n",
    "matrix_size = b.shape[0]\n",
    "G = nk.Graph(matrix_size, weighted = True)\n",
    "\n",
    "print(\"created graph nodes\")\n",
    "\n",
    "for i in range(len(indices)):\n",
    "    G.addEdge(indices[i][0], indices[i][1], b[indices[i][0]][indices[i][1]])\n",
    "\n",
    "print(\"created graph edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nk.overview(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "communities = nk.community.detectCommunities(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scc = nk.components.StronglyConnectedComponents(G)\n",
    "scc.run()\n",
    "print(\"number of components: \", scc.numberOfComponents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_weights = []\n",
    "list_edges = []\n",
    "for u, v, w in G.iterEdgesWeights():\n",
    "    if w > 60000:\n",
    "        list_weights.append(w)\n",
    "        list_edges.append([u,v])\n",
    "#for i range G.degree(1)\n",
    "print(len(list_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(list_edges))\n",
    "for test_idx in range(len(list_edges)):\n",
    "    #if(len(df_clean[\"characters\"].iloc[indices[test_idx][0]]) >= 3 and \n",
    "    #   len(df_clean[\"characters\"].iloc[indices[test_idx][1]]) >= 3 ):\n",
    "    print()\n",
    "    print(list_weights[test_idx])\n",
    "    #print(list_edges[test_idx], list_weights[test_idx])\n",
    "    print(df_clean[\"name\"].iloc[list_edges[test_idx][0]], df_clean[\"release_date\"].iloc[list_edges[test_idx][0]], df_clean[\"wikiID\"].iloc[list_edges[test_idx][0]])\n",
    "    print(df_clean[\"name\"].iloc[list_edges[test_idx][1]], df_clean[\"release_date\"].iloc[list_edges[test_idx][1]], df_clean[\"wikiID\"].iloc[list_edges[test_idx][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx = 8\n",
    "print(df_clean[\"name\"].iloc[list_edges[test_idx][0]], df_clean[\"release_date\"].iloc[list_edges[test_idx][0]], df_clean[\"wikiID\"].iloc[list_edges[test_idx][0]])\n",
    "print(df_clean[\"plot\"].iloc[list_edges[test_idx][0]])\n",
    "print(df_clean[\"name\"].iloc[list_edges[test_idx][1]], df_clean[\"release_date\"].iloc[list_edges[test_idx][1]], df_clean[\"wikiID\"].iloc[list_edges[test_idx][1]])\n",
    "print(df_clean[\"plot\"].iloc[list_edges[test_idx][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tuning the weights using sequels\n",
    "\n",
    "How to tell if our similarity function between two movies is working well? First of all, we can't say it for sure, since it is a subjective question. However usually we can still agree on these points:\n",
    "\n",
    "* Movies that are part of a sequel should have a high similarity\n",
    "* Movies with completely different genres should have a low similarity (for exmaple one is a \"Adventure/Aciton\" movie and the other a \"Romantic Comedy\")\n",
    "\n",
    "Hence we can build two datasets, one of pairs of movies we expect to have a high similarity and the other of pairs of movies we expect to have low similarity. We can then use these datasets to assess if our similarity function is giving the \"right\" values.\n",
    "\n",
    "**Movies from sequels**\n",
    "\n",
    "Before creating pairs of movies that are similar we should group movies by sequels. Unfortunately, we don't have this data directly accessible in our dataset, but by using the name.clusters.txt file we can have characters that are re-used. It doesn't necesseraly mean that the movie is a sequel (for example with Sherlock Holmes it can be the another representation of him playing), but we still expect these movies to be similar.\n",
    "\n",
    "First, using the `df_char` dataframe from character.metadata.tsv and the `freeID_char_map` given for each character, we find the unique `WikiID` movie in which the character is played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char_names[\"WikiID\"] = [ df_char[df_char['freeID_char_map'] == x][\"WikiID\"].item() for x in df_char_names[\"freeID_char_map\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we group all characters together, so for each character we have a list of movies in which he played (represented by their WikiID)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char_grouped = df_char_names.groupby(\"char_name\")[\"WikiID\"].apply(set).to_frame()\n",
    "df_char_grouped.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could stop here, but right now we still have repetitions: for example Harry Potter and Hermione Granger will appear in the same movies, it would be good to group them together to have a list of separate sequels. However we will have a problem: what to do if only some actors repeat? For example if we take \"The Lord of the Rings\" and \"The Hobbit\", Gandalf will appear in both, Aragorn only in \"The Lord of the Rings\" and Thorin Oakenshield only in \"The Hobbit\". In our case, we will assume if some characters repeat in these sequels and others don't, it's still highly likely very similar movies, so we will group them all together.\n",
    "\n",
    "In other words, we group the movies together until we have disjoint movie sets.\n",
    "\n",
    "Because the list of grouped characters isn't that big (970 rows), we can work with python lists instead of the panda dataframe. We create a list of movie sets (each element of the list comes from one character):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_list = []\n",
    "# Iterate over each row\n",
    "for index, rows in df_char_grouped.iterrows():\n",
    "    # append the list to the final list\n",
    "    starting_list.append(rows[\"WikiID\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we join the movies sets from different characters if they both appear in at least one same movie. To do it, we create an empty list, and then iteratively check if we should append a new set (if it has no intersection with the previously added) or make a union with an already existing set (if there is an intersection). To assure that the sets are completely disjoint we run this merging algorithm until the starting \"individual\" list has the same lenght as the \"joined\" list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_characters = []\n",
    "same = False\n",
    "\n",
    "while not(same):    \n",
    "    for i in starting_list:\n",
    "        joined = False\n",
    "        for j in range(len(joined_characters)):\n",
    "            if joined_characters[j] & i: #there is an intersection in the sets\n",
    "                joined_characters[j] = joined_characters[j] | i #the union of both sets\n",
    "                joined = True\n",
    "        if joined == False:\n",
    "            joined_characters.append(i)\n",
    "    \n",
    "    if len(starting_list) == len(joined_characters):\n",
    "        same = True\n",
    "    else:\n",
    "        starting_list = joined_characters\n",
    "        joined_characters = []\n",
    "\n",
    "print(len(joined_characters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get 583 different sequels, which will be enough to test for similarities. To actually compute a similarity we will need to choose a sequel, and then choose a pair of movies in this sequel. If a sequel has n movies in it, the number of different pairs we can build is n*(n-1)/2. If we compute this for all sequels, we find that we can have 10929 different possible pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_pairs = 0\n",
    "for i in joined_characters:\n",
    "    n = len(i)\n",
    "    sum_pairs = sum_pairs + n*(n-1)/2\n",
    "\n",
    "print(sum_pairs, \"different possible pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Movies with different genres**\n",
    "\n",
    "For this milestone we didn't built a dataset of pairs of movies with different genres, because we're confident it won't be a problem to find them. Building a dataset also won't be a problem: you sample a movie randomly, and then sample another one to form a pair. If the second one has an intersection in the genres, you resample it, until you find one with 0 intersection. We won't create all possible pairs because the dataset will be too large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "With this two datasets we will be able to test our similarity function, and possibly adjust its values to have a high similarity in sequels and low similarity in movies from completely different genres. We will also need to be careful to not base the tuning completely on this as by the construction of the datasets it will give a high importance to similar characters and genres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Visualization method\n",
    "\n",
    "We want to be able to visualize our graph. But how do you represent a graph? You could start by placing one node, and then place its neighbours at a distance (or 1/similarity) around him, then place their neighbours and so on. But with that method you can have a problem of not respecting distances with already 3 nodes A, B and C: for example the distance AB = BC = 1 and distance AC = 10. It means we probably won't be able to represent the graph perfectly, but we can still try to do the best possible by plotting nodes with a high similarity close to each other and those with a low similarity far from each other. This is done using Graph Layout Algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gephi\n",
    "\n",
    "<div>\n",
    "<img src=\"images/gephi.jpg\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "**Definition**\n",
    "\n",
    "Gephi is an \"open-source platform for visualizing and manipulating large graphs\". Their Graph Layout Algorithm is force based, meaning nodes will repell and attract each other in function of the distance between them, and the visualization is in the equilibrium state.\n",
    " \n",
    "\n",
    "**Implementation**\n",
    "\n",
    "We found a [github repository](https://github.com/la-rana-kermit/Gephi-python-module) to interface Gephi with Python, which is what we'll try to use to implement. However this repository doesn't seem very active, so if we get some problems we might switch to the Gephi Software directly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annexe: Mathematical definitions related to similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Cosine similarity\n",
    "$$\n",
    "cosine \\: similarity(A,B)=S_c(A,B)=cos(\\theta)=\\frac{A \\cdot B}{\\|A\\| \\|B\\|}=\\frac{\\sum_{i=1}^{n}A_iB_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2}\\sqrt{\\sum_{i=1}^{n} B_i^2}}\n",
    "$$\n",
    "Where $A,B \\in \\mathbb{R}^n$, $S_c(A,B) \\in [-1,1]$ where -1 means that the two vectors are exactly opposite, and 1 means that they are exactly similar and 0 means that they are orthognonal which shows decorrelation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Centered Cosine similarity\n",
    "$$\n",
    "centered \\: cosine \\: similarity(A,B)=\\frac{(A-\\overline{A}) \\cdot (B-\\overline{B})}{\\|A-\\overline{A}\\| \\|B-\\overline{B}\\|}\n",
    "$$\n",
    "Where A and B have been normalized before by substracting their mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Soft cosine similarity\n",
    "$$\n",
    "soft \\: cosine \\: similarity(A,B)=\\frac{\\sum_{i,j}^{n}s_{ij}A_iB_j}{\\sqrt{\\sum_{i,j}^{n} s_{ij}A_iA_j}\\sqrt{\\sum_{i,j}^{n} s_{ij}B_iB_j}}\n",
    "$$\n",
    "where $s_{ij}$=similarity($feature_i$,$feature_j$). For example if $s_{ii}$=1 and $s_{ij}$=0 $\\forall i\\neq j$ then there is no similarity between features, then the soft cosine similarity is equal to the cosine similarity. In the case where features are words, the matrix $S$ has to define the similarity between words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Associated distance with cosine similarity\n",
    "- **Angular distance**\n",
    "\n",
    "if $A_i,B_i \\in \\mathbb{R}$ \n",
    "$$\n",
    "angular \\: distance=D_\\theta=\\frac{arccos(S_c(A,B))}{\\pi}=\\frac{\\theta}{\\pi}\n",
    "$$\n",
    "$$\n",
    "angular \\: similarity=S_\\theta=1-D_\\theta=1-\\frac{\\theta}{\\pi}\n",
    "$$\n",
    "if $A_i,B_i \\in \\mathbb{R}$ and $A_i,B_i\\geq 0$\n",
    "$$\n",
    "angular \\: distance=D_\\theta=\\frac{2 \\cdot \\textrm{arccos}(S_c(A,B))}{\\pi}=\\frac{2\\theta}{\\pi}\n",
    "$$\n",
    "$$\n",
    "angular \\: similarity=S_\\theta=1-D_\\theta=1-\\frac{2\\theta}{\\pi}\n",
    "$$\n",
    "Where the angular distacne is a formal distance metric, however the arccos computation cost makes it more computationally expensive and slower.\n",
    "\n",
    "- **Cosine distance**\n",
    "\n",
    "$$\n",
    "cosine \\: distance=D_c=1-S_c(A,B)\n",
    "$$\n",
    "Where the cosine distance is an unformal distance metric (it does not respect the triangle inequality or Schwarz inequality) but it is less computationally expensive.\n",
    "\n",
    "\n",
    "- **L2-normalized Euclidean distance**\n",
    "\n",
    "From the L2 distance defined as followed: $ \\|x\\|_2=\\sqrt{\\sum x_i^2}=\\sqrt{x.x} $ and the euclidean distance defined as followed: \n",
    "$ d(A,B)=|A-B|=\\sqrt{\\sum _{i=1}^{n} (A_i-B_i)^2 } $, we get the L2-normalized Euclidean distance:\n",
    "$$\n",
    "L2-normalized \\: Euclidean \\: distance=\\sqrt{\\sum _{i=1}^{n} (A_i'-B_i')^2} \\quad \\textrm{where} \\quad A'=\\frac{A}{\\|A\\|_2}\n",
    "$$\n",
    "\n",
    "The cosine similarity and associated distances reflects relative rather than absolute comparison of vectors. For example vectors $A$ and $\\alpha A$ where $\\alpha \\in \\mathbb{R}$ are maximally similar. Therefore this similarity is appropriate for data where frequency is more important than absolute value. For text comparison it can be very useful, we could compare the frequency of terms in a document.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Jaccard similarity coefficient\n",
    "\n",
    "$$\n",
    "J(A,B) = \\frac{|A \\cap B|}{|A \\cup B|} = \\frac{|A \\cap B|}{|A| + |B| - |A \\cap B|}\n",
    "$$\n",
    "where $J(A,B) \\in [0,1]$.\n",
    "It measures similarity between finite sample sets, and is defined as the size of the intersection divided by the size of the union of the sample sets.  \n",
    "\n",
    "#### 1.6 Associated distance with Jaccard similarity\n",
    "$$\n",
    "d_J(A,B)=1-J(A,B)=\\frac{|A\\cup B|-|A\\cap B|}{|A\\cup B|}\n",
    "$$\n",
    "The Jaccard distance, which measures dissimilarity between sample sets, is complementary to the Jaccard coefficient and is obtained by subtracting the Jaccard coefficient from 1, or, equivalently, by dividing the difference of the sizes of the union and the intersection of two sets by the size of the union."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "82c445bf06c8f806e5f5b2ee4d9ad5bd58e46bfb14d70f2dfbba05d708c9be3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
