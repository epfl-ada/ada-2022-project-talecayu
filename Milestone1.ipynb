{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = pd.read_csv(\"data/MovieSummaries/plot_summaries.txt\", sep='\\t', header=None, names=[\"wikiID\", \"plot\"])\n",
    "\n",
    "df_meta = pd.read_csv(\"data/MovieSummaries/movie.metadata.tsv\", sep='\\t', header=None, \n",
    "    names=[\"wikiID\", \"freeID\", \"name\", \"release_date\", \"revenue\", \"runtime\", \"languages\", \"countries\", \"genres\"])\n",
    "\n",
    "df_char = pd.read_csv(\"data/MovieSummaries/character.metadata.tsv\", sep='\\t',header=None, \n",
    "    names=[\"WikiID\", \"freeID\", \"release_date\", \"char_name\", \"actor_DOB\", \"actor_gender\", \"actor_height\", \"actor_ethnicity\",\n",
    "           \"actor_name\", \"Actor_age\", \"freeID_char_map\", \"FreeID_char\", \"FreeID_actor\"])\n",
    "\n",
    "df_char_names = pd.read_csv('data/MovieSummaries/name.clusters.txt', sep=\"\\t\", header=None, \n",
    "    names=[\"char_name\", \"freeID_char_map\"])\n",
    "\n",
    "df_tropes = pd.read_csv('data/MovieSummaries/tvtropes.clusters.txt', sep='\\t', header=None, names=[\"trope\", \"details\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_meta.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char_names.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tropes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tropes = df_tropes.drop([\"details\"], axis=1).join(pd.json_normalize(df_tropes[\"details\"].map(json.loads).tolist())).rename(\n",
    "    columns={\"id\":\"freeID_char_map\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tropes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tropes.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tropes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join the dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature connecting dataframes together is the Wikipedia ID. Also there are more metadatas of movies (81741 movies) than plots (42303 movies). We will only keep the metadatas of the movies we know the plot of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_full = df_meta.merge(df_plot, how='inner', on=\"wikiID\")\n",
    "df_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only a small fraction of the characters have been labellised with a trope (500), compared to the number of unlabeled (450669). Here is the dataframe containing the characters with trope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inner_char = df_char.merge(df_tropes, how='inner', on='freeID_char_map')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inner_char.drop(columns=[\"char\",\"actor\"],inplace=True)\n",
    "df_inner_char.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.dom import minidom\n",
    "\n",
    "# parse an xml file by name\n",
    "file = minidom.parse('data/corenlp_plot_summaries/3217.txt.xml')\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genres, languages and countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcount(df):\n",
    "    df2 = pd.DataFrame(data={\n",
    "        'id': df.apply(lambda x: list(json.loads(x).keys())),\n",
    "        'name': df.apply(lambda x: list(json.loads(x).values()))\n",
    "    })\n",
    "    \n",
    "    distinctf = []\n",
    "    counter = []\n",
    "\n",
    "    for i in range(df2.shape[0]):\n",
    "        fid = df2[\"id\"].iloc[i]\n",
    "        fname = df2[\"name\"].iloc[i]\n",
    "        for j in range(len(fid)):\n",
    "            if [fid[j], fname[j]] not in distinctf:\n",
    "                distinctf.append([fid[j], fname[j]])\n",
    "                counter.append(1)\n",
    "            else:\n",
    "                counter[distinctf.index([fid[j], fname[j]])] += 1\n",
    "                \n",
    "    new = pd.DataFrame(data={\n",
    "        'id': [s[0] for s in distinctf],\n",
    "        'name': [s[1] for s in distinctf],\n",
    "        'count': counter\n",
    "    })\n",
    "    \n",
    "    return new.sort_values(by=\"count\", ascending=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genre = fcount(df_meta[\"genres\"])\n",
    "df_genre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lang = fcount(df_meta[\"languages\"])\n",
    "df_lang.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country = fcount(df_meta[\"countries\"])\n",
    "df_country.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Release date distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta['release_date'] = pd.to_datetime(df_meta['release_date'], errors = 'coerce')\n",
    "df_meta.groupby(df_meta[\"release_date\"].dt.year).count()['wikiID'].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_meta['runtime'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is strange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 300\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.hist(df_meta[df_meta['runtime'] < 300]['runtime'], bins=n_bins)\n",
    "ax.set_title('Distribution of runtime')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of number of words in plot description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use raw plot_summaries and count nb of words in each plot \n",
    "df_plot_copy = df_plot.copy()\n",
    "df_plot_copy['nb_words'] = df_plot_copy['plot'].apply(lambda n: len(n.split()))\n",
    "df_plot_copy.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 1000\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.hist(df_plot_copy['nb_words'], bins=n_bins)\n",
    "ax.set_title('Distribution of number of words per plot description')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep the plots with less than 2000 words (which interval?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2000\n",
    "df_plot_copy = df_plot_copy.loc[df_plot_copy['nb_words'] < threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 1000\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.hist(df_plot_copy['nb_words'], bins=n_bins)\n",
    "ax.set_title('Distribution of number of words per plot description')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would be the minimum number of words to find senseful topic extraction?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* characters by film distribition: how many characters have been labelised for each film?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char_copy = df_char[['freeID','FreeID_actor']].copy()\n",
    "df_char_copy = df_char_copy.groupby(['freeID']).size().reset_index(name='counts')\n",
    "df_char_copy.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 130\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.hist(df_char_copy['counts'], bins=n_bins)\n",
    "ax.set_title('Distribution of number of characters labelised per film')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = 200\n",
    "red_square = dict(markerfacecolor='r', marker='s')\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.boxplot(df_char_copy['counts'],vert=False, flierprops=red_square)\n",
    "ax.set_title('Number of characters labelised per film')\n",
    "plt.show()\n",
    "print(\"1rst quartile, median and 3rd quartile values: \")\n",
    "print(df_char_copy['counts'].quantile([0.25,0.5,0.75]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Hemingway & Gellhorn\" has more than 115 characters labelised! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char_copy.loc[df_char_copy['counts'] > 100].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing of metadata dataset : remove films without country, language, genre, runtime, release_date. \n",
    "Also remove films with less than nb_min_actors (=1) labeled actors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_min_actors = 1\n",
    "#remove Nans\n",
    "df_char_noNaN = df_char.loc[df_char['char_name'].isna() | (df_char['actor_name'].isna())]\n",
    "#group by movies the nb of actors labelised by movie\n",
    "df_clean_char_noNan = df_char_noNaN.groupby(['freeID']).size().reset_index(name='actor count')\n",
    "#remove the films with less than nb_min_actors\n",
    "df_clean_char = df_clean_char_noNan.loc[df_clean_char_noNan['actor count'] > nb_min_actors]\n",
    "\n",
    "print('number of movies remaining after character dataset preprocessing: ', df_clean_char.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inner_meta = df_meta.merge(df_clean_char, how='inner', on='freeID')\n",
    "\n",
    "df_clean = df_inner_meta.loc[(df_inner_meta['countries'] != '{}') & (df_inner_meta['languages'] != '{}') & (df_inner_meta['genres'] != '{}') &\n",
    "            df_inner_meta['release_date'].notnull()].reset_index()\n",
    "    \n",
    "print('number of movies remaining after movie metadata preprocessing: ', df_clean.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.merge(df_plot, how='inner', on='wikiID')\n",
    "\n",
    "print('number of movies remaining after movie metadata preprocessing: ', df_clean.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By observing the non-null counts, on the runtime and revenue columns, we decided to eliminate these attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.drop(columns=['revenue','runtime','index'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right formatting for next data analysis steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['countries'] = pd.DataFrame(data={\n",
    "        #'id': df_clean['countries'].apply(lambda x: list(json.loads(x).keys())),\n",
    "        'countries': df_clean['countries'].apply(lambda x: list(json.loads(x).values()))\n",
    "    })\n",
    "df_clean['languages'] = pd.DataFrame(data={\n",
    "        #'id': df_clean['countries'].apply(lambda x: list(json.loads(x).keys())),\n",
    "        'languages': df_clean['languages'].apply(lambda x: list(json.loads(x).values()))\n",
    "    })\n",
    "df_clean['genres'] = pd.DataFrame(data={\n",
    "        #'id': df_clean['countries'].apply(lambda x: list(json.loads(x).keys())),\n",
    "        'genres': df_clean['genres'].apply(lambda x: list(json.loads(x).values()))\n",
    "    })    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessed dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every columns has the same number of non-null values, and there isn't any null value inside of it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "\n",
    "path = \"data/corenlp_plot_summaries/\"\n",
    "files = os.listdir(path)\n",
    "print(files[0])\n",
    "f = gzip.open(path+files[0], 'rb')\n",
    "\n",
    "test = f.read().decode()\n",
    "# print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "Bs_data = BeautifulSoup(test, \"xml\")\n",
    " \n",
    "print(Bs_data.find(\"sentence\", {'id':'20'}).find('NER').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of attributes based on data exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attributs scraped:\n",
    "-Director \n",
    "-Color\n",
    "-Runtime to complete dataset?\n",
    " \n",
    "enlever attribut du dataset final:\n",
    "- Revenu \n",
    "\n",
    "filtré?\n",
    "- Title\n",
    "- Plot existant + taille plot\n",
    "- Genre\n",
    "\n",
    "non filtré? \n",
    "- Runtime:  remplacer valeur abérente par non mais pas filtré\n",
    "- Release date: remplacer valeur abérente par non mais pas filtré\n",
    "- Revenu\n",
    "- Language\n",
    "- Country\n",
    "- Actors\n",
    "- Characters\n",
    "- Director\n",
    "- Color\n",
    "\n",
    "retenu pour similarité? \n",
    "-Plot\n",
    "-Title\n",
    "-Runtime: long metrage/court metrage qui remplace run time\n",
    "-Release date: year\n",
    "-Country (petit poids)\n",
    "-Language (petit poids)\n",
    "-Actors\n",
    "-Characters\n",
    "-Genres\n",
    "-Director\n",
    "-Color (petit poids)\n",
    "\n",
    "pas retenu pour similarité? \n",
    "- Revenu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods to study similarities accross chosen attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Mathematical similarity definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Cosine similarity\n",
    "$$\n",
    "cosine \\: similarity(A,B)=S_c(A,B)=cos(\\theta)=\\frac{A.B}{\\|A\\| \\|B\\|}=\\frac{\\sum_{i=1}^{n}A_iB_i}{\\sqrt{\\sum_{i=1}^{n} A_i}\\sqrt{\\sum_{i=1}^{n} B_i}}\n",
    "$$\n",
    "Where $A,B \\in \\mathbb{R}^n$, $S_c(A,B) \\in [-1,1]$ where -1 means that the two vectors are exactly opposite, and 1 means that they are exactly similar and 0 means that they are orthognonal which shows decorrelation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Centered Cosine similarity\n",
    "$$\n",
    "centered \\: cosine \\: similarity(A,B)=\\frac{(A-\\overline{A}).(B-\\overline{B})}{\\|A-\\overline{A}\\| \\|B-\\overline{B}\\|}\n",
    "$$\n",
    "Where A and B have been normalized before by substracting their mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Associated distance with cosine similarity\n",
    "- **Angular distance**\n",
    "\n",
    "if $A_i,B_i \\in \\mathbb{R}$ \n",
    "$$\n",
    "angular \\: distance=D_\\theta=\\frac{arccos(S_c(A,B))}{\\pi}=\\frac{\\theta}{\\pi}\n",
    "$$\n",
    "$$\n",
    "angular \\: similarity=S_\\theta=1-D_\\theta=1-\\frac{\\theta}{\\pi}\n",
    "$$\n",
    "if $A_i,B_i \\in \\mathbb{R}$ and $A_i,B_i\\geq 0$\n",
    "$$\n",
    "angular \\: distance=D_\\theta=\\frac{2.arccos(S_c(A,B))}{\\pi}=\\frac{2\\theta}{\\pi}\n",
    "$$\n",
    "$$\n",
    "angular \\: similarity=S_\\theta=1-D_\\theta=1-\\frac{2\\theta}{\\pi}\n",
    "$$\n",
    "Where the angular distacne is a formal distance metric, however the arccos computation cost makes it more computationally expensive and slower.\n",
    "\n",
    "- **Cosine distance**\n",
    "\n",
    "$$\n",
    "cosine \\: distance=D_c=1-S_c(A,B)\n",
    "$$\n",
    "Where the cosine distance is an unformal distance metric (it does not respect the triangle inequality or Schwarz inequality) but it is less computationally expensive.\n",
    "\n",
    "\n",
    "- **L2-normalized Euclidean distance**\n",
    "\n",
    "From the L2 distance defined as followed: $ \\|x\\|_2=\\sqrt{\\sum x_i^2}=\\sqrt{x.x} $ and the euclidean distance defined as followed: \n",
    "$ d(A,B)=|A-B|=\\sqrt{\\sum _{i=1}^{n} (A_i-B_i)^2 } $, we get the L2-normalized Euclidean distance:\n",
    "$$\n",
    "L2-normalized \\: Euclidean \\: distance=\\sqrt{\\sum _{i=1}^{n} (A_i'-B_i')^2} \\quad \\textrm{where} \\quad A'=\\frac{A}{\\|A\\|_2}\n",
    "$$\n",
    "\n",
    "The cosine similarity and associated distances reflects relative rather than absolute comparison of vectors. For example vectors $A$ and $\\alpha A$ where $\\alpha \\in \\mathbb{R}$ are maximally similar. Therefore this similarity is appropriate for data where frequency is more important than absolute value. For text comparison it can be very useful, we could compare the frequency of terms in a document.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Soft cosine similarity\n",
    "$$\n",
    "soft \\: cosine \\: similarity(A,B)=\\frac{\\sum_{i,j}^{n}s_{ij}A_iB_j}{\\sqrt{\\sum_{i,j}^{n} s_{ij}A_iA_j}\\sqrt{\\sum_{i,j}^{n} s_{ij}B_iB_j}}\n",
    "$$\n",
    "where $s_{ij}$=similarity($feature_i$,$feature_j$). For example if $s_{ii}$=1 and $s_{ij}$=0 $\\forall i\\neq j$ then there is no similarity between features, then the soft cosine similarity is equal to the cosine similarity. In the case where features are words, the matrix $S$ has to define the similarity between words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Algorithm to study similarity between movies attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf-idf : Term frequency-inverse document frequency\n",
    "**Term frequency and Inverse document frequency**\n",
    "\n",
    "$$\n",
    "tf(t,d)=\\frac{f_{t,d}}{ \\sum_{t' \\in d} f_{t',d} } \\quad \\textrm{and} \\quad idf(t,D)=log(\\frac{N}{|d\\in D:t\\in d|})\n",
    "$$\n",
    "where $f_{t,d}$ = raw frequency = number of times a term $t$ occurs in document $d$, $ \\sum_{t' \\in d} f_{t',d}$=total number of terms $t'$ in $d$ by suming each independent occurrence\n",
    "<br>\n",
    "where $N$=$|D|$= number of documents in corpus $D$, $|d\\in D:t\\in d|$= number of documents where t appears (where $tf(t,d)\\neq 0$)\n",
    "\n",
    "To prevent bias towards longer documents, the term frequency can be computed as follow:\n",
    "$$\n",
    "tf(t,d)=0.5+0.5\\frac{f_{t,d}}{max(f_{t',d}:t'\\in d)}\n",
    "$$\n",
    "To avoid divinding by zero, the idf denominator can ba adjusted as follow:\n",
    "$$\n",
    "idf(t,D)=log(\\frac{N}{1+|d\\in D:t\\in d|})\n",
    "$$\n",
    "**Term frequency-inverse document frequency**\n",
    "\n",
    "From the term frequency and the inverse document frequency we can compute the tf-idf:\n",
    "$$\n",
    "tf-idf(t,D)=tf(t,d).idf(t,D)\n",
    "$$\n",
    "\n",
    "The tf-idf reflects how important a word is to a document in corpus. tf-idf increase proportionally with the number of times a word appear in a document, but it is offset by the number of documents in the corpus that contains the word. It is high when there is high term frequency in document d **and** low document frequency of the term in the whole corpus. This helps to adjust the fact that some words appear more frequently that we can define as common terms. It tends to filter out those common words.\n",
    "\n",
    "**Implementation**\n",
    "\n",
    "tfidf can be implemented using the `GenSim` opensource library. After obtaining tf-idf associated for each word in each text, we can compute the cosine similarity between those texts. Since we compare to what extent different movies contain the exact same words, we need to add a lemmatizer filtering to take care of the same words being in the singular or plural form.\n",
    "xxx\n",
    "\n",
    "**Movie Attribute similarity that can be studied**\n",
    "- Movie genre\n",
    "- Movie Language ¨\n",
    "- Actor\n",
    "- Characters \n",
    "- Movie Country??? xxx\n",
    "- Director??? xxx \n",
    "\n",
    "Here we can use this method for these attributes because we want to find which of those attributes contain the exact same words and study the similarity based on this analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GloVe algorithm\n",
    "**Definition**\n",
    "The `GloVe` algorithm is an opensource standford algorithm that associates similarity between words by obtaining vectors representations for words and by mapping them into a meaningful space.\n",
    "\n",
    "**Implementation** \n",
    "We can apply soft cosine similarity computation to study similarity between texts. The matrix S from the soft cosine definition can be calculated with the library `GenSim` using the `GloVe` algorithm. We would obtain a dataset containing a vector for all the words in our corpus and we could compute the matrix S associating a similarity matrix to all of these words. Then with the soft cosine similarity we could analyse the similarity between the documents in our corpus. \n",
    "\n",
    "**Movie Attribute similarity that can be studied**\n",
    "- Title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing method on a document corpus before topic extraction\n",
    "\n",
    "Here is a preprocessing approach that can be implemented before using a topic extraction analysis\n",
    "1. Removing the stop words from the documents which are the most common words occuring in texts that give no additional concept. It can be done with `Java` using `MySQL`.\n",
    "2. Removing the numbers appart from years, the non-asci characters, and most common occuring names (ex: James, Robert, John)\n",
    "3. Handle pural and singular form of the same word by lemmatizing.\n",
    "4. We can filter the words using tf-idf. We can compute the tf-idf for each word of each plot and keep the words with highest tf-idf score. The threshold for a word to be kept has to be defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA: Latent Dirichlet Allocation\n",
    "\n",
    "<div>\n",
    "<img src=\"images/LDA_example.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "**Definition**\n",
    "\n",
    "LDA is an algorithm that can be used for topic extraction in texts. It is an unsupervised machine-learning model that takes documents as input and finds topics as output. A topic is represented as a weighted list of words. The model also says in what percentage each document talks about each topic. \n",
    "\n",
    "**Implementation**\n",
    "\n",
    "LDA can be implemented using the `GenSim` library. When using LDA to analyse topics in a corpus, it needs some preprocessing steps before applying the algorithm to make it more efficient. We detailed one preprocessing approach above.\n",
    "\n",
    "\n",
    "After this processing we can apply LDA on our database by tuning some parameters:\n",
    "<br>\n",
    "$K$: the number of topics we look for\n",
    "<br>\n",
    "$\\alpha$: K-dimension vector of positive reals that represent the prior weights of topic K in a document which affects the document-topic distribution. \n",
    "<br>\n",
    "$\\eta$: V-dimension vector of positive reals that represents the prior weights of each words in topics which affects the topic-word distribution\n",
    "\n",
    "If we chose a symetric LDA, the weights $\\alpha$ would be the same for all topics and the weights $\\eta$ would be the same for all words in a topic. The smaller the $\\alpha$ the fewer topics per document, the fewer the $\\eta$ the fewer words per topic.\n",
    "\n",
    "**Movie Attribute similarity that can be studied**\n",
    "- Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doc2Vec topic extraction\n",
    "**Definition**\n",
    "\n",
    "Doc2Vec is an unsupervised algorithm that learns fixed-length feature vectors for paragraphs/documents/texts. Then we can compare these vectors to assess the similarity between documents. Doc2vec allows to generate a semantic space which is a spatial space where distance among vectors are indicator of semantic similarity. This semantic space consisting of word and document vectors is a continuous representation of topics, unlike LDA where topics are sampled from a discrete space. It means that the dense areas having high concentration of document can be thought of having similar topics and can be best represented by nearby embedded words.\n",
    "\n",
    "**Implementation**\n",
    "\n",
    "It can be implemented using the `GenSim` library with the class `Doc2Vec` that extends the class `Word2Vec`. \n",
    "\n",
    "\n",
    "**Movie Attribute similarity that can be studied**\n",
    "- Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Release year similarity algorithm to add xxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Assessing the global similarity between movies taking into account all attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding weights between attributes, to make the global similarity depending more or less on specific attributes\n",
    "\n",
    "xxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method to merge all attributes similarities together\n",
    "\n",
    "xxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tuning the weights using sequels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to tell if our similarity function between two movies is working well? First of all, we can't say it for sure, since it is a subjective question. However usually we can still agree on these points:\n",
    "\n",
    "- Movies that are part of a sequel should have a high similarity\n",
    "- Movies with completely different genres should have a low similarity (for exmaple one is a \"Adventure/Aciton\" movie and the other a \"Romantic Comedy\")\n",
    "\n",
    "Hence we can build two datasets, one of pairs of movies we expect to have a high similarity and the other of pairs of movies we expect to have low similarity. We can then use these datasets to assess if our similarity function is giving the \"right\" values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Movies from sequels**\n",
    "\n",
    "Before creating pairs of movies that are similar we should group movies by sequels. Unfortunately, we don't have this data directly accessible in our dataset, but by using the `name.clusters.txt` file we can have characters that are re-used. It doesn't necesseraly mean that the movie is a sequel (for example with Sherlock Holmes it can be the another representation of him playing), but we still expect these movies to be similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, using the `character.metadata.tsv` and the `freeID_char_map` given for each character, we find the movie in which the character is played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char_names[\"WikiID\"] = [ df_char[df_char['freeID_char_map'] == x][\"WikiID\"].item() for x in df_char_names[\"freeID_char_map\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char_grouped = df_char_names.groupby(\"char_name\")[\"WikiID\"].apply(set).to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_list = []\n",
    "# Iterate over each row\n",
    "for index, rows in df_char_grouped.iterrows():\n",
    "    # append the list to the final list\n",
    "    starting_list.append(rows[\"WikiID\"])\n",
    "print(starting_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_characters = []\n",
    "same = False\n",
    "\n",
    "while not(same):    \n",
    "    for i in starting_list:\n",
    "        joined = False\n",
    "        for j in range(len(joined_characters)):\n",
    "            if joined_characters[j] & i:\n",
    "                joined_characters[j] = joined_characters[j] | i\n",
    "                joined = True\n",
    "        if joined == False:\n",
    "            joined_characters.append(i)\n",
    "    #print(starting_list)\n",
    "    #print(joined_characters)\n",
    "    if len(starting_list) == len(joined_characters):\n",
    "        same = True\n",
    "    else:\n",
    "        starting_list = joined_characters\n",
    "        joined_characters = []\n",
    "\n",
    "print(len(joined_characters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find movies from freeId_char_map using the `character.metadata.tsv` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char_grouped = df_char_names.groupby(\"char_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char_grouped.first()\n",
    "#df_char_names.head()\n",
    "#iterate over characters\n",
    "#iterate over char_ids\n",
    "#find movie wikiID for a char_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find movies from freeId_char_map\n",
    "\n",
    "if movie has no plot, remove it from group\n",
    "\n",
    "if group has only one movie (because of removed before), remove it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Movies with different genres**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE\n",
    "**Definition**\n",
    "\n",
    "**Implementation**\n",
    "\n",
    "`scikit-learn` library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Recommandation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbors\n",
    "**Definition**\n",
    "\n",
    "**Implementation**\n",
    "\n",
    "`scikit-learn` library\n",
    "<br>\n",
    "tensorboard associated with `TensorFlow` library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "82c445bf06c8f806e5f5b2ee4d9ad5bd58e46bfb14d70f2dfbba05d708c9be3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
