{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot = pd.read_csv(\"data/MovieSummaries/plot_summaries.txt\", sep='\\t', header=None, names=[\"wikiID\", \"plot\"])\n",
    "\n",
    "df_meta = pd.read_csv(\"data/MovieSummaries/movie.metadata.tsv\", sep='\\t', header=None, \n",
    "    names=[\"wikiID\", \"freeID\", \"name\", \"release_date\", \"revenue\", \"runtime\", \"languages\", \"countries\", \"genres\"])\n",
    "\n",
    "df_char = pd.read_csv(\"data/MovieSummaries/character.metadata.tsv\", sep='\\t',header=None, \n",
    "    names=[\"WikiID\", \"freeID\", \"release_date\", \"char_name\", \"actor_DOB\", \"actor_gender\", \"actor_height\", \"actor_ethnicity\",\n",
    "           \"actor_name\", \"Actor_age\", \"freeID_char_map\", \"FreeID_char\", \"FreeID_actor\"])\n",
    "\n",
    "df_char_names = pd.read_csv('data/MovieSummaries/name.clusters.txt', sep=\"\\t\", header=None, \n",
    "    names=[\"char_name\", \"freeID_char_map\"])\n",
    "\n",
    "df_tropes = pd.read_csv('data/MovieSummaries/tvtropes.clusters.txt', sep='\\t', header=None, names=[\"trope\", \"details\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data features and missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the output above, we might think that the languages, countries and genres columns have no missing elements but actually some elements are just empty dictionary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_meta['languages'].value_counts()['{}'])\n",
    "print(df_meta['countries'].value_counts()['{}'])\n",
    "print(df_meta['genres'].value_counts()['{}'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char_names.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tropes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tropes = df_tropes.drop([\"details\"], axis=1).join(pd.json_normalize(df_tropes[\"details\"].map(json.loads).tolist())).rename(\n",
    "    columns={\"id\":\"freeID_char_map\"})\n",
    "\n",
    "df_tropes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tropes.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## NLPCore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we looked at the Stanford CoreNLP-processed summaries, we decided it would not be useful for us.\n",
    "\n",
    "The code below shows how to treat these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "path = \"data/corenlp_plot_summaries/\"\n",
    "files = os.listdir(path)\n",
    "print(files[0])\n",
    "f = gzip.open(path+files[0], 'rb')\n",
    "\n",
    "test = f.read().decode()\n",
    "# print(test)\n",
    "\n",
    "Bs_data = BeautifulSoup(test, \"xml\")\n",
    "print(Bs_data.find(\"sentence\", {'id':'20'}).find('NER').text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genres, languages and countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcount(df):\n",
    "    df2 = pd.DataFrame(data={\n",
    "        'id': df.apply(lambda x: list(json.loads(x).keys())),\n",
    "        'name': df.apply(lambda x: list(json.loads(x).values()))\n",
    "    })\n",
    "    \n",
    "    distinctf = []\n",
    "    counter = []\n",
    "\n",
    "    for i in range(df2.shape[0]):\n",
    "        fid = df2[\"id\"].iloc[i]\n",
    "        fname = df2[\"name\"].iloc[i]\n",
    "        for j in range(len(fid)):\n",
    "            if [fid[j], fname[j]] not in distinctf:\n",
    "                distinctf.append([fid[j], fname[j]])\n",
    "                counter.append(1)\n",
    "            else:\n",
    "                counter[distinctf.index([fid[j], fname[j]])] += 1\n",
    "\n",
    "    new = pd.DataFrame(data={\n",
    "        'id': [s[0] for s in distinctf],\n",
    "        'name': [s[1] for s in distinctf],\n",
    "        'count': counter\n",
    "    })\n",
    "    \n",
    "    return new.sort_values(by=\"count\", ascending=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genre = fcount(df_meta[\"genres\"])\n",
    "df_genre.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lang = fcount(df_meta[\"languages\"])\n",
    "df_lang.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country = fcount(df_meta[\"countries\"])\n",
    "df_country.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Release date distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_meta['release_date'].dt.year.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta['release_date'] = pd.to_datetime(df_meta['release_date'], errors = 'coerce')\n",
    "df_meta.groupby(df_meta[\"release_date\"].dt.year).count()['wikiID'].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_meta['runtime'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 years of runtime?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.hist(df_meta[df_meta['runtime'] < 400]['runtime'], bins=400)\n",
    "ax.set_title('Distribution of runtime')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runtime distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_meta['revenue'].dropna().hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_meta['revenue'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_meta[df_meta['revenue']==df_meta['revenue'].max()].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a very large different between most of the movies and the movies that had a huge success such as \"Avatar\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of number of words in plot description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use raw plot_summaries and count nb of words in each plot \n",
    "df_plot_fltr = df_plot.copy()\n",
    "df_plot_fltr['nb_words'] = df_plot_fltr['plot'].apply(lambda n: len(n.split()))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.hist(df_plot_fltr['nb_words'], bins=1000)\n",
    "ax.set_title('Distribution of number of words per plot description')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep the plots with less than 2000 words (which interval?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2000\n",
    "df_plot_fltr = df_plot_fltr.loc[df_plot_fltr['nb_words'] < threshold]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.hist(df_plot_fltr['nb_words'], bins=1000)\n",
    "ax.set_title('Distribution of number of words per plot description')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would be the minimum number of words to find senseful topic extraction?  \n",
    "* characters by film distribition: how many characters have been labelised for each film?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char_fltr = df_char[['freeID','FreeID_actor']].copy()\n",
    "df_char_fltr = df_char_fltr.groupby(['freeID']).size().reset_index(name='counts')\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.hist(df_char_fltr['counts'], bins=120)\n",
    "ax.set_title('Distribution of number of characters labelised per film')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_square = dict(markerfacecolor='r', marker='s')\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.boxplot(df_char_fltr['counts'], vert=False, flierprops=red_square)\n",
    "ax.set_title('Number of characters labelised per film')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"1rst quartile, median and 3rd quartile values: \")\n",
    "print(df_char_fltr['counts'].quantile([0.25,0.5,0.75]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Hemingway & Gellhorn\" has more than 115 characters labelised! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char_fltr.loc[df_char_fltr['counts'] > 100].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join the dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature connecting dataframes together is the Wikipedia ID. Also there are more metadatas of movies (81741 movies) than plots (42303 movies). We will only keep the metadatas of the movies we know the plot of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_full = df_meta.merge(df_plot, how='inner', on=\"wikiID\")\n",
    "df_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only a small fraction of the characters have been labellised with a trope (500), compared to the number of unlabeled (450669). Here is the dataframe containing the characters with trope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inner_char = df_char.merge(df_tropes, how='inner', on='freeID_char_map')\n",
    "\n",
    "df_inner_char.drop(columns=[\"char\", \"actor\"], inplace=True)\n",
    "df_inner_char.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing of metadata dataset : remove films without country, language, genre, runtime, release_date. \n",
    "Also remove films with less than nb_min_actors (=1) labeled actors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_min_actors = 1\n",
    "# Remove NaNs\n",
    "# df_char_noNaN = df_char.loc[df_char['char_name'].isna() | (df_char['actor_name'].isna())]\n",
    "df_char_ = df_char.dropna(subset=['char_name', 'actor_name'])\n",
    "# Group by movies the nb of actors labelised by movie\n",
    "df_char_count = df_char_.groupby(['freeID']).size().reset_index(name='actor count')\n",
    "# Remove the films with less than nb_min_actors\n",
    "df_char_clean = df_char_count.loc[df_char_count['actor count'] >= nb_min_actors]\n",
    "\n",
    "print('number of movies remaining after character dataset preprocessing: ', df_char_clean.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inner_meta = df_meta.merge(df_char_clean, how='inner', on='freeID')\n",
    "\n",
    "df_clean = df_inner_meta.loc[(df_inner_meta['countries'] != '{}') & (df_inner_meta['languages'] != '{}') & \n",
    "                             (df_inner_meta['genres'] != '{}') & df_inner_meta['release_date'].notnull()].reset_index()\n",
    "\n",
    "print('number of movies remaining after movie metadata preprocessing: ', df_clean.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_clean.merge(df_plot, how='inner', on='wikiID')\n",
    "\n",
    "print('number of movies remaining after movie metadata preprocessing: ', df_clean.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By observing the non-null counts, on the runtime and revenue columns, we decided to eliminate these attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean.drop(columns=['revenue','runtime','index'], inplace=True)\n",
    "df_clean.drop(columns=['revenue','index'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right formatting for next data analysis steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean['countries'] = pd.DataFrame(data={'countries': df_clean['countries'].apply(lambda x: list(json.loads(x).values()))})\n",
    "df_clean['languages'] = pd.DataFrame(data={'languages': df_clean['languages'].apply(lambda x: list(json.loads(x).values()))})\n",
    "df_clean['genres'] = pd.DataFrame(data={'genres': df_clean['genres'].apply(lambda x: list(json.loads(x).values()))})    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessed dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every columns has the same number of non-null values, and there isn't any null value inside of it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pywikibot\n",
    "\n",
    "\n",
    "def scrapping(wiki_ID):\n",
    "    list = []\n",
    "\n",
    "    query_base = \"https://query.wikidata.org/bigdata/namespace/wdq/sparql?format=json&query=\"\n",
    "\n",
    "    wiki_ID_encoded = wiki_ID.replace(\"/\",\"%2F\")\n",
    "\n",
    "    pre_str = \"https://query.wikidata.org/bigdata/namespace/wdq/sparql?format=json&query=PREFIX%20wd%3A%20%3Chttp%3A%2F%2Fwww.wikidata.org%2Fentity%2F%3E%0APREFIX%20wdt%3A%20%3Chttp%3A%2F%2Fwww.wikidata.org%2Fprop%2Fdirect%2F%3E%0APREFIX%20wikibase%3A%20%3Chttp%3A%2F%2Fwikiba.se%2Fontology%23%3E%0A%0ASELECT%20%20%3Fs%20%3FsLabel%20%3Fp%20%20%3Fo%20%3FoLabel%20WHERE%20%7B%0A%20%3Fs%20wdt%3AP646%20%22\"\n",
    "    post_str = \"%22%20%0A%0A%20%20%20SERVICE%20wikibase%3Alabel%20%7B%0A%20%20%20%20bd%3AserviceParam%20wikibase%3Alanguage%20%22en%22%20.%0A%20%20%20%7D%0A%20%7D\"\n",
    "\n",
    "    query = pre_str + wiki_ID_encoded + post_str\n",
    "\n",
    "    response = requests.get(query)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    results = soup.find(id=\"results\")\n",
    "\n",
    "    try:\n",
    "        entity_wiki_id = response.json()['results']['bindings'][0]['s']['value']\n",
    "    except:\n",
    "        print(\"Scrapping failed\")\n",
    "        return [None,None,None,None,None]\n",
    "        \n",
    "\n",
    "    entity_wiki_id = response.json()['results']['bindings'][0]['s']['value']\n",
    "\n",
    "    str = entity_wiki_id.split('/')\n",
    "    entity = str[-1]\n",
    "    print(entity)\n",
    "    site = pywikibot.Site(\"wikidata\", \"wikidata\")\n",
    "    repo = site.data_repository()\n",
    "    item = pywikibot.ItemPage(repo, entity)\n",
    "    item_dict = item.get()\n",
    "    clm_dict = item_dict[\"claims\"] # Get the claim dictionary\n",
    "\n",
    "    #P57 for a movie director\n",
    "    try:\n",
    "        clm_list = clm_dict[\"P57\"]\n",
    "        for clm in clm_list:\n",
    "                ...\n",
    "                clm_trgt = clm.getTarget()\n",
    "        list.append(clm_trgt.labels['en'])\n",
    "    except:\n",
    "        list.append(None)\n",
    "\n",
    "    #P86 for a composer\n",
    "    try:\n",
    "        clm_list = clm_dict[\"P86\"]\n",
    "        for clm in clm_list:\n",
    "                ...\n",
    "                clm_trgt = clm.getTarget()\n",
    "        list.append(clm_trgt.labels['en'])\n",
    "    except:\n",
    "        list.append(None)\n",
    "\n",
    "    #P840 for the narrative location\n",
    "    try:\n",
    "        clm_list = clm_dict[\"P840\"]\n",
    "        for clm in clm_list:\n",
    "                ...\n",
    "                clm_trgt = clm.getTarget()\n",
    "        list.append(clm_trgt.labels['en'])\n",
    "    except:\n",
    "        list.append(None)\n",
    "\n",
    "    #P344 for the photographer\n",
    "    try:\n",
    "        clm_list = clm_dict[\"P344\"]\n",
    "        for clm in clm_list:\n",
    "                ...\n",
    "                clm_trgt = clm.getTarget()\n",
    "        list.append(clm_trgt.labels['en'])\n",
    "    except:\n",
    "        list.append(None)\n",
    "\n",
    "    #P462 for the color\n",
    "    try:\n",
    "        clm_list = clm_dict[\"P462\"]\n",
    "        for clm in clm_list:\n",
    "                ...\n",
    "                clm_trgt = clm.getTarget()\n",
    "        list.append(clm_trgt.labels['en'])\n",
    "    except:\n",
    "        list.append(None)\n",
    "\n",
    "    print('scraped sucessfuly')\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = df_clean.iloc[0:15000]\n",
    "\n",
    "list2 = [[]]\n",
    "for index, row in sub_df.iterrows():\n",
    "    list2.append(scrapping(row['freeID']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Choice of attributes based on data exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attributs scraped:\n",
    "- Director \n",
    "- Color\n",
    "- Runtime to complete dataset?\n",
    " \n",
    "enlever attribut du dataset final:\n",
    "- Revenu \n",
    "\n",
    "filtré?\n",
    "- Title\n",
    "- Plot existant + taille plot\n",
    "- Genre\n",
    "\n",
    "non filtré? \n",
    "- Runtime:  remplacer valeur abérente par non mais pas filtré\n",
    "- Release date: remplacer valeur abérente par non mais pas filtré\n",
    "- Revenu\n",
    "- Language\n",
    "- Country\n",
    "- Actors\n",
    "- Characters\n",
    "- Director\n",
    "- Color\n",
    "\n",
    "retenu pour similarité? \n",
    "- Plot\n",
    "- Title\n",
    "- Runtime: long metrage/court metrage qui remplace run time\n",
    "- Release date: year\n",
    "- Country (petit poids)\n",
    "- Language (petit poids)\n",
    "- Actors\n",
    "- Characters\n",
    "- Genres\n",
    "- Director\n",
    "- Color (petit poids)\n",
    "\n",
    "pas retenu pour similarité? \n",
    "- Revenu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Centered Cosine similarity\n",
    "$$\n",
    "centered \\: cosine \\: similarity(A,B)=\\frac{(A-\\overline{A}) \\cdot (B-\\overline{B})}{\\|A-\\overline{A}\\| \\|B-\\overline{B}\\|}\n",
    "$$\n",
    "Where A and B have been normalized before by substracting their mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Soft cosine similarity\n",
    "$$\n",
    "soft \\: cosine \\: similarity(A,B)=\\frac{\\sum_{i,j}^{n}s_{ij}A_iB_j}{\\sqrt{\\sum_{i,j}^{n} s_{ij}A_iA_j}\\sqrt{\\sum_{i,j}^{n} s_{ij}B_iB_j}}\n",
    "$$\n",
    "where $s_{ij}$=similarity($feature_i$,$feature_j$). For example if $s_{ii}$=1 and $s_{ij}$=0 $\\forall i\\neq j$ then there is no similarity between features, then the soft cosine similarity is equal to the cosine similarity. In the case where features are words, the matrix $S$ has to define the similarity between words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Associated distance with cosine similarity\n",
    "- **Angular distance**\n",
    "\n",
    "if $A_i,B_i \\in \\mathbb{R}$ \n",
    "$$\n",
    "angular \\: distance=D_\\theta=\\frac{arccos(S_c(A,B))}{\\pi}=\\frac{\\theta}{\\pi}\n",
    "$$\n",
    "$$\n",
    "angular \\: similarity=S_\\theta=1-D_\\theta=1-\\frac{\\theta}{\\pi}\n",
    "$$\n",
    "if $A_i,B_i \\in \\mathbb{R}$ and $A_i,B_i\\geq 0$\n",
    "$$\n",
    "angular \\: distance=D_\\theta=\\frac{2 \\cdot \\textrm{arccos}(S_c(A,B))}{\\pi}=\\frac{2\\theta}{\\pi}\n",
    "$$\n",
    "$$\n",
    "angular \\: similarity=S_\\theta=1-D_\\theta=1-\\frac{2\\theta}{\\pi}\n",
    "$$\n",
    "Where the angular distacne is a formal distance metric, however the arccos computation cost makes it more computationally expensive and slower.\n",
    "\n",
    "- **Cosine distance**\n",
    "\n",
    "$$\n",
    "cosine \\: distance=D_c=1-S_c(A,B)\n",
    "$$\n",
    "Where the cosine distance is an unformal distance metric (it does not respect the triangle inequality or Schwarz inequality) but it is less computationally expensive.\n",
    "\n",
    "\n",
    "- **L2-normalized Euclidean distance**\n",
    "\n",
    "From the L2 distance defined as followed: $ \\|x\\|_2=\\sqrt{\\sum x_i^2}=\\sqrt{x.x} $ and the euclidean distance defined as followed: \n",
    "$ d(A,B)=|A-B|=\\sqrt{\\sum _{i=1}^{n} (A_i-B_i)^2 } $, we get the L2-normalized Euclidean distance:\n",
    "$$\n",
    "L2-normalized \\: Euclidean \\: distance=\\sqrt{\\sum _{i=1}^{n} (A_i'-B_i')^2} \\quad \\textrm{where} \\quad A'=\\frac{A}{\\|A\\|_2}\n",
    "$$\n",
    "\n",
    "The cosine similarity and associated distances reflects relative rather than absolute comparison of vectors. For example vectors $A$ and $\\alpha A$ where $\\alpha \\in \\mathbb{R}$ are maximally similar. Therefore this similarity is appropriate for data where frequency is more important than absolute value. For text comparison it can be very useful, we could compare the frequency of terms in a document.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.5 Jaccard similarity coefficient\n",
    "\n",
    "$$\n",
    "J(A,B) = \\frac{|A \\cap B|}{|A \\cup B|} = \\frac{|A \\cap B|}{|A| + |B| - |A \\cap B|}\n",
    "$$\n",
    "where $J(A,B) \\in [0,1]$.\n",
    "It measures similarity between finite sample sets, and is defined as the size of the intersection divided by the size of the union of the sample sets.  \n",
    "\n",
    "#### 1.6 Associated distance with Jaccard similarity\n",
    "$$\n",
    "d_J(A,B)=1-J(A,B)=\\frac{|A\\cup B|-|A\\cap B|}{|A\\cup B|}\n",
    "$$\n",
    "The Jaccard distance, which measures dissimilarity between sample sets, is complementary to the Jaccard coefficient and is obtained by subtracting the Jaccard coefficient from 1, or, equivalently, by dividing the difference of the sizes of the union and the intersection of two sets by the size of the union."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Algorithm to study similarity between movies attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import seaborn as sns\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Jaccard similarity implementation\n",
    "**Method**\n",
    "The jaccard similarity is more appropriate to compute similarities between datasets of categorical attributes where we want to see if these datasets have some attributes in common or not, and how many they are.\n",
    "\n",
    "**Implementation**\n",
    "`jaccard_similarity` function written here below. Since we compare to what extent different movies attributes contain the exact same words, we may need to add a lemmatizer for example to take care of same words being in the singular or plural form, or having uppercase or lowercase initial letter.\n",
    "\n",
    "**Movie Attribute similarity that can be studied**\n",
    "- Movie genre\n",
    "- Movie Language \n",
    "- Movie Country\n",
    "- Actors\n",
    "- Characters\n",
    "\n",
    "Here we can use this method for these attributes because we want to find which of those attributes contain the exact same words and study the similarity based on this analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(x,y):\n",
    "  \"\"\" returns the jaccard similarity between two lists \"\"\"\n",
    "  intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
    "  union_cardinality = len(set.union(*[set(x), set(y)]))\n",
    "  return intersection_cardinality/float(union_cardinality)\n",
    "\n",
    "print(\"Example of similarities computed with jaccard similarity for the attribute -Movie genre- :\")\n",
    "list1 = ['Drama', 'Mystery', 'Fantasy', 'Adventure']\n",
    "list2 = ['Fantasy Adventure', 'Adventure', 'Epic', 'Fantasy', 'Drama', 'Action', 'Sword and sorcery films']\n",
    "list3= ['Satire', 'Comedy', 'Family film']\n",
    "list4= ['Musical', 'Drama', 'Comedy']\n",
    "list5= ['Romantic comedy', 'Romance Film', 'Comedy']\n",
    "list6= ['Comedy']\n",
    "list7=[]\n",
    "list8= ['Action']\n",
    "\n",
    "print(jaccard_similarity(list1, list2))\n",
    "print(jaccard_similarity(list3, list4))\n",
    "print(jaccard_similarity(list5, list6))\n",
    "print(jaccard_similarity(list3, list6))\n",
    "print(jaccard_similarity(list3, list7))\n",
    "print(jaccard_similarity(list2, list8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf-idf : Term frequency-inverse document frequency\n",
    "**Term frequency and Inverse document frequency**\n",
    "\n",
    "$$\n",
    "tf(t,d)=\\frac{f_{t,d}}{ \\sum_{t' \\in d} f_{t',d} } \\quad \\textrm{and} \\quad idf(t,D)=log(\\frac{N}{|d\\in D:t\\in d|})\n",
    "$$\n",
    "where $f_{t,d}$ = raw frequency = number of times a term $t$ occurs in document $d$, $ \\sum_{t' \\in d} f_{t',d}$=total number of terms $t'$ in $d$ by suming each independent occurrence\n",
    "<br>\n",
    "where $N$=$|D|$= number of documents in corpus $D$, $|d\\in D:t\\in d|$= number of documents where t appears (where $tf(t,d)\\neq 0$)\n",
    "\n",
    "To prevent bias towards longer documents, the term frequency can be computed as follow:\n",
    "$$\n",
    "tf(t,d)=0.5+0.5\\frac{f_{t,d}}{max(f_{t',d}:t'\\in d)}\n",
    "$$\n",
    "To avoid divinding by zero, the idf denominator can ba adjusted as follow:\n",
    "$$\n",
    "idf(t,D)=log(\\frac{N}{1+|d\\in D:t\\in d|})\n",
    "$$\n",
    "**Term frequency-inverse document frequency**\n",
    "\n",
    "From the term frequency and the inverse document frequency we can compute the tf-idf:\n",
    "$$\n",
    "tf-idf(t,D)=tf(t,d) \\cdot idf(t,D)\n",
    "$$\n",
    "\n",
    "The tf-idf reflects how important a word is to a document in corpus. tf-idf increase proportionally with the number of times a word appear in a document, but it is offset by the number of documents in the corpus that contains the word. It is high when there is high term frequency in document d **and** low document frequency of the term in the whole corpus. This helps to adjust the fact that some words appear more frequently that we can define as common terms. It tends to filter out those common words.\n",
    "\n",
    "**Implementation**\n",
    "\n",
    "tfidf can be implemented using the `sklearn` or `GenSim` opensource library. After obtaining tf-idf associated for each word in each text, we can associate to each text a vector composed of his words tf-idf and null values for other words.\n",
    "Then we can compute the **cosine similarity** between those texts represented as vectors.\n",
    "\n",
    "**Movie Attribute similarity that can be studied**\n",
    "- Title??? xxx\n",
    "\n",
    "Here we can use this method for these attributes because we want to find which of those attributes contain the exact same words and study the similarity based on this analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(A,B): \n",
    "  return (sum(a*b for a,b in zip(A,B)))\n",
    "\n",
    "def cosine_similarity(a,b):\n",
    "  \"\"\" returns cosine similarity between two lists \"\"\"\n",
    "  return dot(a,b)/((dot(a,a)**.5)*(dot(b,b)**.5)) \n",
    "\n",
    "print(cosine_similarity([1,0],[1,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#list1= 'Drama Mystery Fantasy Adventure'\n",
    "#list2= 'Fantasy Adventure  Adventure  Epic  Fantasy  Drama  Action Sword and sorcery'\n",
    "#list3= 'Satire Comedy Family'\n",
    "#list4= 'Musical Drama Comedy'\n",
    "#list5= 'Romantic Comedy Romance Comedy'\n",
    "#list6= 'Comedy'\n",
    "\n",
    "list1= 'Harry Potter and the Chamber of Secrets'\n",
    "list2= 'Gnomes and Trolls: The Secret Chamber'\n",
    "list3= 'Harry Potter and the Half-Blood Prince'\n",
    "list4= 'The Taking of Prince Harry'\n",
    "list5= 'The Prince and Me'\n",
    "list6= 'Despicable Me'\n",
    "list7= 'Just Me and You'\n",
    "list8= 'The Secret Life of Bees'\n",
    "headlines = [list1,list2,list3,list4,list5,list6,list7,list8]\n",
    "\n",
    "labels = [headline[:30] for headline in headlines] \n",
    "def create_heatmap(similarity, cmap = \"YlGnBu\"):\n",
    "  df = pd.DataFrame(similarity)\n",
    "  df.columns = labels\n",
    "  df.index = labels\n",
    "  fig, ax = plt.subplots(figsize=(5,5))\n",
    "  sns.heatmap(df, cmap=cmap)\n",
    "\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(headlines)\n",
    "#print(vectorizer.get_feature_names())\n",
    "list_vectors = X.toarray()\n",
    "print(\"cosine similarities between 6 lists: \\n\",cosine_similarity(list_vectors))\n",
    "create_heatmap(cosine_similarity(list_vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GloVe algorithm\n",
    "**Definition**\n",
    "The `GloVe` algorithm is an opensource standford algorithm that associates similarity between words by obtaining vectors representations for words and by mapping them into a meaningful space.\n",
    "\n",
    "**Implementation** \n",
    "We can apply soft cosine similarity computation to study similarity between texts. The matrix S from the soft cosine definition can be calculated with the library `GenSim` and using the `GloVe` algorithm. We would obtain a dataset containing a vector for all the words in our corpus and we could compute the matrix S associating a similarity matrix to all of these words. Then with the soft cosine similarity we could analyse the similarity between the documents in our corpus. \n",
    "\n",
    "**Movie Attribute similarity that can be studied**\n",
    "- Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import sub\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "query= 'Harry Potter and the Chamber of Secrets'\n",
    "list2= 'Gnomes and Trolls: The Secret Chamber'\n",
    "list3= 'Harry Potter and the Half-Blood Prince'\n",
    "list4= 'The Taking of Prince Harry'\n",
    "list5= 'The Prince and Me'\n",
    "list6= 'Despicable Me'\n",
    "documents = [list2,list3,list4,list5,list6]\n",
    "stopwords = ['the', 'and', 'are', 'a']\n",
    "\n",
    "# From: https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb\n",
    "def preprocess(doc):\n",
    "    # Tokenize, clean up input document string\n",
    "    doc = sub(r'<img[^<>]+(>|$)', \" image_token \", doc)\n",
    "    doc = sub(r'<[^<>]+(>|$)', \" \", doc)\n",
    "    doc = sub(r'\\[img_assist[^]]*?\\]', \" \", doc)\n",
    "    doc = sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', \" url_token \", doc)\n",
    "    return [token for token in simple_preprocess(doc, min_len=0, max_len=float(\"inf\")) if token not in stopwords]\n",
    "\n",
    "# Preprocess the documents, including the query string\n",
    "corpus = [preprocess(document) for document in documents]\n",
    "query = preprocess(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models import WordEmbeddingSimilarityIndex\n",
    "from gensim.similarities import SparseTermSimilarityMatrix\n",
    "from gensim.similarities import SoftCosineSimilarity\n",
    "\n",
    "# Load the model: this is a big file, can take a while to download and open\n",
    "glove = api.load(\"glove-wiki-gigaword-50\")    \n",
    "similarity_index = WordEmbeddingSimilarityIndex(glove)\n",
    "\n",
    "# Build the term dictionary, TF-idf model\n",
    "dictionary = Dictionary(corpus+[query])\n",
    "tfidf = TfidfModel(dictionary=dictionary)\n",
    "\n",
    "# Create the term similarity matrix.  \n",
    "similarity_matrix = SparseTermSimilarityMatrix(similarity_index, dictionary, tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Soft Cosine Measure between the query and the documents.\n",
    "# From: https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/soft_cosine_tutorial.ipynb\n",
    "query_tf = tfidf[dictionary.doc2bow(query)]\n",
    "\n",
    "index = SoftCosineSimilarity(tfidf[[dictionary.doc2bow(document) for document in corpus]],similarity_matrix)\n",
    "\n",
    "doc_similarity_scores = index[query_tf]\n",
    "\n",
    "# Output the sorted similarity scores and documents\n",
    "sorted_indexes = np.argsort(doc_similarity_scores)[::-1]\n",
    "for idx in sorted_indexes:\n",
    "    print(f'{idx} \\t {doc_similarity_scores[idx]:0.3f} \\t {documents[idx]}')\n",
    "\n",
    "# 1    0.688    tomatoes are actually fruit\n",
    "# 0    0.000    cars drive on the road"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.similarities import docsim\n",
    "from docsim import DocSim\n",
    "docsim = DocSim_threaded(verbose=True)\n",
    "similarities = docsim.similarity_query(query_string, documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic extraction method\n",
    "To compare plots, we will use a topic extraction algorithm. There are a lot that exist, but we will concentrate on three possible: LDA, Doc2Vec and BERTopic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing method on a document corpus before topic extraction\n",
    "\n",
    "Here is a preprocessing approach that can be implemented before using a topic extraction analysis\n",
    "1. Removing the stop words from the documents which are the most common words occuring in texts that give no additional concept. It can be done with `Java` using `MySQL`.\n",
    "2. Removing the numbers appart from years, the non-asci characters, and most common occuring names (ex: James, Robert, John)\n",
    "3. Handle pural and singular form of the same word by lemmatizing.\n",
    "4. We can filter the words using tf-idf. We can compute the tf-idf for each word of each plot and keep the words with highest tf-idf score. The threshold for a word to be kept has to be defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA: Latent Dirichlet Allocation\n",
    "\n",
    "<div>\n",
    "<img src=\"images/LDA_example.png\" width=\"800\"/>\n",
    "</div>\n",
    "\n",
    "**Definition**\n",
    "\n",
    "LDA is an algorithm that can be used for topic extraction in texts. It is an unsupervised machine-learning model that takes documents as input and finds topics as output. A topic is represented as a weighted list of words. The model also says in what percentage each document talks about each topic. \n",
    "\n",
    "**Implementation**\n",
    "\n",
    "LDA can be implemented using the `GenSim` library. When using LDA to analyse topics in a corpus, it needs some preprocessing steps before applying the algorithm to make it more efficient. We detailed one preprocessing approach above.\n",
    "\n",
    "\n",
    "After this processing we can apply LDA on our database by tuning some parameters:\n",
    "<br>\n",
    "$K$: the number of topics we look for\n",
    "<br>\n",
    "$\\alpha$: K-dimension vector of positive reals that represent the prior weights of topic K in a document which affects the document-topic distribution. \n",
    "<br>\n",
    "$\\eta$: V-dimension vector of positive reals that represents the prior weights of each words in topics which affects the topic-word distribution\n",
    "\n",
    "If we chose a symetric LDA, the weights $\\alpha$ would be the same for all topics and the weights $\\eta$ would be the same for all words in a topic. The smaller the $\\alpha$ the fewer topics per document, the fewer the $\\eta$ the fewer words per topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERTopic\n",
    "\n",
    "<div>\n",
    "<img src=\"images/BERTopic.png\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "**Definition**\n",
    "BERTopic is a guided semi-supervised topic modeling algorithm. Contrary to LDA, it uses embeddings (thus semi-supervised) and class-based tf-idf (ctf-idf) to find easily interpretable topics and makes it more stable to small variations. It works in three stages:\n",
    "\n",
    "- Embed the documents\n",
    "- Cluster the documents\n",
    "- Create a list of topics and their representation\n",
    "\n",
    "With this algorithm it is possible to find a list of topics and also give the probability of each topic in each document.\n",
    "\n",
    "**Implementation**\n",
    "\n",
    "BERTopic can be implemented using the python `BerTopic` library. As with LDA, we can preprocess the documents (plots in our case) before giving it to the BERTopic for better results.\n",
    "\n",
    "Each stage of the BERTopic can be parameterized to get better results:\n",
    "<br>\n",
    "$Embedding$: by default it uses the `paraphrase-MiniLM-L6-v2` sentence transformers, but we can use any other embedding technique.\n",
    "<br>\n",
    "$Clustering$: by default it uses HDBSCAN (hierarchical DBSCAN) to cluster the documents, but to apply this efficiently UMAP is used to reduce the dimensionality of the embeddings. It is possible to change the dimension reduction algorithm and the clustering algorithm, for example we could import K-means form `sklearn.cluster` and use that.\n",
    "<br>\n",
    "$Topic$ $representation$: by default it uses class-based tf-idf, meaning it adjusts the tf-idf inside each class (cluster) in relation with the frequency of a word inside the class. Here again it can be tuned for example to try to show different representatives of a cluster. This can be useful to avoid having \"bycycle\", \"cycling\" and \"bike\" as the representatives of a cluster, where only one of them would have been enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Release year algorithm to add -> sim inverse prop to difference\n",
    "#### Release director -> same or different attribute? similarity 1/0\n",
    "#### Runtime (court/lon-gmetrage) + color similarity -> same or different attribute? similarity 1/0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Assessing the global similarity between movies taking into account all attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding weights between attributes, to make the global similarity depending more or less on specific attributes\n",
    "\n",
    "xxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method to merge all attributes similarities together\n",
    "\n",
    "xxx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tuning the weights using sequels\n",
    "\n",
    "How to tell if our similarity function between two movies is working well? First of all, we can't say it for sure, since it is a subjective question. However usually we can still agree on these points:\n",
    "\n",
    "* Movies that are part of a sequel should have a high similarity\n",
    "* Movies with completely different genres should have a low similarity (for exmaple one is a \"Adventure/Aciton\" movie and the other a \"Romantic Comedy\")\n",
    "\n",
    "Hence we can build two datasets, one of pairs of movies we expect to have a high similarity and the other of pairs of movies we expect to have low similarity. We can then use these datasets to assess if our similarity function is giving the \"right\" values.\n",
    "\n",
    "**Movies from sequels**\n",
    "\n",
    "Before creating pairs of movies that are similar we should group movies by sequels. Unfortunately, we don't have this data directly accessible in our dataset, but by using the name.clusters.txt file we can have characters that are re-used. It doesn't necesseraly mean that the movie is a sequel (for example with Sherlock Holmes it can be the another representation of him playing), but we still expect these movies to be similar.\n",
    "\n",
    "First, using the `df_char` dataframe from character.metadata.tsv and the `freeID_char_map` given for each character, we find the unique `WikiID` movie in which the character is played."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char_names[\"WikiID\"] = [ df_char[df_char['freeID_char_map'] == x][\"WikiID\"].item() for x in df_char_names[\"freeID_char_map\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we group all characters together, so for each character we have a list of movies in which he played (represented by their WikiID)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_char_grouped = df_char_names.groupby(\"char_name\")[\"WikiID\"].apply(set).to_frame()\n",
    "df_char_grouped.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could stop here, but right now we still have repetitions: for example Harry Potter and Hermione Granger will appear in the same movies, it would be good to group them together to have a list of separate sequels. However we will have a problem: what to do if only some actors repeat? For example if we take \"The Lord of the Rings\" and \"The Hobbit\", Gandalf will appear in both, Aragorn only in \"The Lord of the Rings\" and Thorin Oakenshield only in \"The Hobbit\". In our case, we will assume if some characters repeat in these sequels and others don't, it's still highly likely very similar movies, so we will group them all together.\n",
    "\n",
    "In other words, we group the movies together until we have disjoint movie sets.\n",
    "\n",
    "Because the list of grouped characters isn't that big (970 rows), we can work with python lists instead of the panda dataframe. We create a list of movie sets (each element of the list comes from one character):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_list = []\n",
    "# Iterate over each row\n",
    "for index, rows in df_char_grouped.iterrows():\n",
    "    # append the list to the final list\n",
    "    starting_list.append(rows[\"WikiID\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we join the movies sets from different characters if they both appear in at least one same movie. To do it, we create an empty list, and then iteratively check if we should append a new set (if it has no intersection with the previously added) or make a union with an already existing set (if there is an intersection). To assure that the sets are completely disjoint we run this merging algorithm until the starting \"individual\" list has the same lenght as the \"joined\" list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_characters = []\n",
    "same = False\n",
    "\n",
    "while not(same):    \n",
    "    for i in starting_list:\n",
    "        joined = False\n",
    "        for j in range(len(joined_characters)):\n",
    "            if joined_characters[j] & i: #there is an intersection in the sets\n",
    "                joined_characters[j] = joined_characters[j] | i #the union of both sets\n",
    "                joined = True\n",
    "        if joined == False:\n",
    "            joined_characters.append(i)\n",
    "    \n",
    "    if len(starting_list) == len(joined_characters):\n",
    "        same = True\n",
    "    else:\n",
    "        starting_list = joined_characters\n",
    "        joined_characters = []\n",
    "\n",
    "print(len(joined_characters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get 583 different sequels, which will be enough to test for similarities. To actually compute a similarity we will need to choose a sequel, and then choose a pair of movies in this sequel. If a sequel has n movies in it, the number of different pairs we can build is n*(n-1)/2. If we compute this for all sequels, we find that we can have 10929 different possible pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_pairs = 0\n",
    "for i in joined_characters:\n",
    "    n = len(i)\n",
    "    sum_pairs = sum_pairs + n*(n-1)/2\n",
    "\n",
    "print(sum_pairs, \"different possible pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Movies with different genres**\n",
    "\n",
    "For this milestone we didn't built a dataset of pairs of movies with different genres, because we're confident it won't be a problem to find them. Building a dataset also won't be a problem: you sample a movie randomly, and then sample another one to form a pair. If the second one has an intersection in the genres, you resample it, until you find one with 0 intersection. We won't create all possible pairs because the dataset will be too large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "With this two datasets we will be able to test our similarity function, and possibly adjust its values to have a high similarity in sequels and low similarity in movies from completely different genres. We will also need to be careful to not base the tuning completely on this as by the construction of the datasets it will give a high importance to similar characters and genres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Visualization method\n",
    "\n",
    "We want to be able to visualize our graph. But how do you represent a graph? You could start by placing one node, and then place its neighbours at a distance (or 1/similarity) around him, then place their neighbours and so on. But with that method you can have a problem of not respecting distances with already 3 nodes A, B and C: for example the distance AB = BC = 1 and distance AC = 10. It means we probably won't be able to represent the graph perfectly, but we can still try to do the best possible by plotting nodes with a high similarity close to each other and those with a low similarity far from each other. This is done using Graph Layout Algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gephi\n",
    "\n",
    "<div>\n",
    "<img src=\"images/gephi.jpg\" width=\"600\"/>\n",
    "</div>\n",
    "\n",
    "**Definition**\n",
    "\n",
    "Gephi is an \"open-source platform for visualizing and manipulating large graphs\". Their Graph Layout Algorithm is force based, meaning nodes will repell and attract each other in function of the distance between them, and the visualization is in the equilibrium state.\n",
    " \n",
    "\n",
    "**Implementation**\n",
    "\n",
    "We found a [github repository](https://github.com/la-rana-kermit/Gephi-python-module) to interface Gephi with Python, which is what we'll try to use to implement. However this repository doesn't seem very active, so if we get some problems we might switch to the Gephi Software directly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## User Recommandation method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Nearest Neighbors\n",
    "**Definition**\n",
    "\n",
    "**Implementation**\n",
    "\n",
    "`scikit-learn` library\n",
    "<br>\n",
    "tensorboard associated with `TensorFlow` library"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "82c445bf06c8f806e5f5b2ee4d9ad5bd58e46bfb14d70f2dfbba05d708c9be3a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
